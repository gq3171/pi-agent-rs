#![allow(clippy::approx_constant)]
use once_cell::sync::Lazy;
use pi_agent_core::types::{Model, ModelCost};
use serde_json::json;
use std::collections::HashMap;

pub static ALL_MODELS: Lazy<Vec<Model>> = Lazy::new(|| all_models());

pub fn all_models() -> Vec<Model> {
    vec![
        Model {
            id: "amazon.nova-2-lite-v1:0".into(),
            name: "Nova 2 Lite".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.33,
                output: 2.75,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "amazon.nova-lite-v1:0".into(),
            name: "Nova Lite".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.06,
                output: 0.24,
                cache_read: 0.015,
                cache_write: 0.0,
            },
            context_window: 300000,
            max_tokens: 8192,
            headers: None,
            compat: None,
        },
        Model {
            id: "amazon.nova-micro-v1:0".into(),
            name: "Nova Micro".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.035,
                output: 0.14,
                cache_read: 0.00875,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 8192,
            headers: None,
            compat: None,
        },
        Model {
            id: "amazon.nova-premier-v1:0".into(),
            name: "Nova Premier".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 2.5,
                output: 12.5,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 1000000,
            max_tokens: 16384,
            headers: None,
            compat: None,
        },
        Model {
            id: "amazon.nova-pro-v1:0".into(),
            name: "Nova Pro".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.8,
                output: 3.2,
                cache_read: 0.2,
                cache_write: 0.0,
            },
            context_window: 300000,
            max_tokens: 8192,
            headers: None,
            compat: None,
        },
        Model {
            id: "amazon.titan-text-express-v1".into(),
            name: "Titan Text G1 - Express".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.2,
                output: 0.6,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "amazon.titan-text-express-v1:0:8k".into(),
            name: "Titan Text G1 - Express".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.2,
                output: 0.6,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "anthropic.claude-3-5-haiku-20241022-v1:0".into(),
            name: "Claude Haiku 3.5".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.8,
                output: 4.0,
                cache_read: 0.08,
                cache_write: 1.0,
            },
            context_window: 200000,
            max_tokens: 8192,
            headers: None,
            compat: None,
        },
        Model {
            id: "anthropic.claude-3-5-sonnet-20240620-v1:0".into(),
            name: "Claude Sonnet 3.5".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 3.0,
                output: 15.0,
                cache_read: 0.3,
                cache_write: 3.75,
            },
            context_window: 200000,
            max_tokens: 8192,
            headers: None,
            compat: None,
        },
        Model {
            id: "anthropic.claude-3-5-sonnet-20241022-v2:0".into(),
            name: "Claude Sonnet 3.5 v2".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 3.0,
                output: 15.0,
                cache_read: 0.3,
                cache_write: 3.75,
            },
            context_window: 200000,
            max_tokens: 8192,
            headers: None,
            compat: None,
        },
        Model {
            id: "anthropic.claude-3-7-sonnet-20250219-v1:0".into(),
            name: "Claude Sonnet 3.7".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 3.0,
                output: 15.0,
                cache_read: 0.3,
                cache_write: 3.75,
            },
            context_window: 200000,
            max_tokens: 8192,
            headers: None,
            compat: None,
        },
        Model {
            id: "anthropic.claude-3-haiku-20240307-v1:0".into(),
            name: "Claude Haiku 3".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.25,
                output: 1.25,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 200000,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "anthropic.claude-3-opus-20240229-v1:0".into(),
            name: "Claude Opus 3".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 15.0,
                output: 75.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 200000,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "anthropic.claude-3-sonnet-20240229-v1:0".into(),
            name: "Claude Sonnet 3".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 3.0,
                output: 15.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 200000,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "anthropic.claude-haiku-4-5-20251001-v1:0".into(),
            name: "Claude Haiku 4.5".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.0,
                output: 5.0,
                cache_read: 0.1,
                cache_write: 1.25,
            },
            context_window: 200000,
            max_tokens: 64000,
            headers: None,
            compat: None,
        },
        Model {
            id: "anthropic.claude-opus-4-1-20250805-v1:0".into(),
            name: "Claude Opus 4.1".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 15.0,
                output: 75.0,
                cache_read: 1.5,
                cache_write: 18.75,
            },
            context_window: 200000,
            max_tokens: 32000,
            headers: None,
            compat: None,
        },
        Model {
            id: "anthropic.claude-opus-4-20250514-v1:0".into(),
            name: "Claude Opus 4".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 15.0,
                output: 75.0,
                cache_read: 1.5,
                cache_write: 18.75,
            },
            context_window: 200000,
            max_tokens: 32000,
            headers: None,
            compat: None,
        },
        Model {
            id: "anthropic.claude-opus-4-5-20251101-v1:0".into(),
            name: "Claude Opus 4.5".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 5.0,
                output: 25.0,
                cache_read: 0.5,
                cache_write: 6.25,
            },
            context_window: 200000,
            max_tokens: 64000,
            headers: None,
            compat: None,
        },
        Model {
            id: "anthropic.claude-opus-4-6-v1".into(),
            name: "Claude Opus 4.6".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 5.0,
                output: 25.0,
                cache_read: 0.5,
                cache_write: 6.25,
            },
            context_window: 200000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "anthropic.claude-sonnet-4-20250514-v1:0".into(),
            name: "Claude Sonnet 4".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 3.0,
                output: 15.0,
                cache_read: 0.3,
                cache_write: 3.75,
            },
            context_window: 200000,
            max_tokens: 64000,
            headers: None,
            compat: None,
        },
        Model {
            id: "anthropic.claude-sonnet-4-5-20250929-v1:0".into(),
            name: "Claude Sonnet 4.5".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 3.0,
                output: 15.0,
                cache_read: 0.3,
                cache_write: 3.75,
            },
            context_window: 200000,
            max_tokens: 64000,
            headers: None,
            compat: None,
        },
        Model {
            id: "cohere.command-r-plus-v1:0".into(),
            name: "Command R+".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 3.0,
                output: 15.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "cohere.command-r-v1:0".into(),
            name: "Command R".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.5,
                output: 1.5,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "deepseek.r1-v1:0".into(),
            name: "DeepSeek-R1".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 1.35,
                output: 5.4,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 32768,
            headers: None,
            compat: None,
        },
        Model {
            id: "deepseek.v3-v1:0".into(),
            name: "DeepSeek-V3.1".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.58,
                output: 1.68,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 163840,
            max_tokens: 81920,
            headers: None,
            compat: None,
        },
        Model {
            id: "eu.anthropic.claude-haiku-4-5-20251001-v1:0".into(),
            name: "Claude Haiku 4.5 (EU)".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.0,
                output: 5.0,
                cache_read: 0.1,
                cache_write: 1.25,
            },
            context_window: 200000,
            max_tokens: 64000,
            headers: None,
            compat: None,
        },
        Model {
            id: "eu.anthropic.claude-opus-4-5-20251101-v1:0".into(),
            name: "Claude Opus 4.5 (EU)".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 5.0,
                output: 25.0,
                cache_read: 0.5,
                cache_write: 6.25,
            },
            context_window: 200000,
            max_tokens: 64000,
            headers: None,
            compat: None,
        },
        Model {
            id: "eu.anthropic.claude-opus-4-6-v1".into(),
            name: "Claude Opus 4.6 (EU)".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 5.0,
                output: 25.0,
                cache_read: 0.5,
                cache_write: 6.25,
            },
            context_window: 200000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "eu.anthropic.claude-sonnet-4-20250514-v1:0".into(),
            name: "Claude Sonnet 4 (EU)".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 3.0,
                output: 15.0,
                cache_read: 0.3,
                cache_write: 3.75,
            },
            context_window: 200000,
            max_tokens: 64000,
            headers: None,
            compat: None,
        },
        Model {
            id: "eu.anthropic.claude-sonnet-4-5-20250929-v1:0".into(),
            name: "Claude Sonnet 4.5 (EU)".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 3.0,
                output: 15.0,
                cache_read: 0.3,
                cache_write: 3.75,
            },
            context_window: 200000,
            max_tokens: 64000,
            headers: None,
            compat: None,
        },
        Model {
            id: "global.anthropic.claude-haiku-4-5-20251001-v1:0".into(),
            name: "Claude Haiku 4.5 (Global)".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.0,
                output: 5.0,
                cache_read: 0.1,
                cache_write: 1.25,
            },
            context_window: 200000,
            max_tokens: 64000,
            headers: None,
            compat: None,
        },
        Model {
            id: "global.anthropic.claude-opus-4-5-20251101-v1:0".into(),
            name: "Claude Opus 4.5 (Global)".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 5.0,
                output: 25.0,
                cache_read: 0.5,
                cache_write: 6.25,
            },
            context_window: 200000,
            max_tokens: 64000,
            headers: None,
            compat: None,
        },
        Model {
            id: "global.anthropic.claude-opus-4-6-v1".into(),
            name: "Claude Opus 4.6 (Global)".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 5.0,
                output: 25.0,
                cache_read: 0.5,
                cache_write: 6.25,
            },
            context_window: 200000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "global.anthropic.claude-sonnet-4-20250514-v1:0".into(),
            name: "Claude Sonnet 4 (Global)".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 3.0,
                output: 15.0,
                cache_read: 0.3,
                cache_write: 3.75,
            },
            context_window: 200000,
            max_tokens: 64000,
            headers: None,
            compat: None,
        },
        Model {
            id: "global.anthropic.claude-sonnet-4-5-20250929-v1:0".into(),
            name: "Claude Sonnet 4.5 (Global)".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 3.0,
                output: 15.0,
                cache_read: 0.3,
                cache_write: 3.75,
            },
            context_window: 200000,
            max_tokens: 64000,
            headers: None,
            compat: None,
        },
        Model {
            id: "google.gemma-3-27b-it".into(),
            name: "Google Gemma 3 27B Instruct".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.12,
                output: 0.2,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 202752,
            max_tokens: 8192,
            headers: None,
            compat: None,
        },
        Model {
            id: "google.gemma-3-4b-it".into(),
            name: "Gemma 3 4B IT".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.04,
                output: 0.08,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "meta.llama3-1-70b-instruct-v1:0".into(),
            name: "Llama 3.1 70B Instruct".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.72,
                output: 0.72,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "meta.llama3-1-8b-instruct-v1:0".into(),
            name: "Llama 3.1 8B Instruct".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.22,
                output: 0.22,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "meta.llama3-2-11b-instruct-v1:0".into(),
            name: "Llama 3.2 11B Instruct".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.16,
                output: 0.16,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "meta.llama3-2-1b-instruct-v1:0".into(),
            name: "Llama 3.2 1B Instruct".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.1,
                output: 0.1,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131000,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "meta.llama3-2-3b-instruct-v1:0".into(),
            name: "Llama 3.2 3B Instruct".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.15,
                output: 0.15,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131000,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "meta.llama3-2-90b-instruct-v1:0".into(),
            name: "Llama 3.2 90B Instruct".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.72,
                output: 0.72,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "meta.llama3-3-70b-instruct-v1:0".into(),
            name: "Llama 3.3 70B Instruct".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.72,
                output: 0.72,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "meta.llama4-maverick-17b-instruct-v1:0".into(),
            name: "Llama 4 Maverick 17B Instruct".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.24,
                output: 0.97,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 1000000,
            max_tokens: 16384,
            headers: None,
            compat: None,
        },
        Model {
            id: "meta.llama4-scout-17b-instruct-v1:0".into(),
            name: "Llama 4 Scout 17B Instruct".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.17,
                output: 0.66,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 3500000,
            max_tokens: 16384,
            headers: None,
            compat: None,
        },
        Model {
            id: "minimax.minimax-m2".into(),
            name: "MiniMax M2".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.3,
                output: 1.2,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 204608,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "mistral.ministral-3-14b-instruct".into(),
            name: "Ministral 14B 3.0".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.2,
                output: 0.2,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "mistral.ministral-3-8b-instruct".into(),
            name: "Ministral 3 8B".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.15,
                output: 0.15,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "mistral.mistral-large-2402-v1:0".into(),
            name: "Mistral Large (24.02)".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.5,
                output: 1.5,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "mistral.voxtral-mini-3b-2507".into(),
            name: "Voxtral Mini 3B 2507".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.04,
                output: 0.04,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "mistral.voxtral-small-24b-2507".into(),
            name: "Voxtral Small 24B 2507".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.15,
                output: 0.35,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 32000,
            max_tokens: 8192,
            headers: None,
            compat: None,
        },
        Model {
            id: "moonshot.kimi-k2-thinking".into(),
            name: "Kimi K2 Thinking".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.6,
                output: 2.5,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 256000,
            max_tokens: 256000,
            headers: None,
            compat: None,
        },
        Model {
            id: "nvidia.nemotron-nano-12b-v2".into(),
            name: "NVIDIA Nemotron Nano 12B v2 VL BF16".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.2,
                output: 0.6,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "nvidia.nemotron-nano-9b-v2".into(),
            name: "NVIDIA Nemotron Nano 9B v2".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.06,
                output: 0.23,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai.gpt-oss-120b-1:0".into(),
            name: "gpt-oss-120b".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.15,
                output: 0.6,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai.gpt-oss-20b-1:0".into(),
            name: "gpt-oss-20b".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.07,
                output: 0.3,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai.gpt-oss-safeguard-120b".into(),
            name: "GPT OSS Safeguard 120B".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.15,
                output: 0.6,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai.gpt-oss-safeguard-20b".into(),
            name: "GPT OSS Safeguard 20B".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.07,
                output: 0.2,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "qwen.qwen3-235b-a22b-2507-v1:0".into(),
            name: "Qwen3 235B A22B 2507".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.22,
                output: 0.88,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 262144,
            max_tokens: 131072,
            headers: None,
            compat: None,
        },
        Model {
            id: "qwen.qwen3-32b-v1:0".into(),
            name: "Qwen3 32B (dense)".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.15,
                output: 0.6,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 16384,
            max_tokens: 16384,
            headers: None,
            compat: None,
        },
        Model {
            id: "qwen.qwen3-coder-30b-a3b-v1:0".into(),
            name: "Qwen3 Coder 30B A3B Instruct".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.15,
                output: 0.6,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 262144,
            max_tokens: 131072,
            headers: None,
            compat: None,
        },
        Model {
            id: "qwen.qwen3-coder-480b-a35b-v1:0".into(),
            name: "Qwen3 Coder 480B A35B Instruct".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.22,
                output: 1.8,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 65536,
            headers: None,
            compat: None,
        },
        Model {
            id: "qwen.qwen3-next-80b-a3b".into(),
            name: "Qwen/Qwen3-Next-80B-A3B-Instruct".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.14,
                output: 1.4,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 262000,
            max_tokens: 262000,
            headers: None,
            compat: None,
        },
        Model {
            id: "qwen.qwen3-vl-235b-a22b".into(),
            name: "Qwen/Qwen3-VL-235B-A22B-Instruct".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.3,
                output: 1.5,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 262000,
            max_tokens: 262000,
            headers: None,
            compat: None,
        },
        Model {
            id: "us.anthropic.claude-haiku-4-5-20251001-v1:0".into(),
            name: "Claude Haiku 4.5 (US)".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.0,
                output: 5.0,
                cache_read: 0.1,
                cache_write: 1.25,
            },
            context_window: 200000,
            max_tokens: 64000,
            headers: None,
            compat: None,
        },
        Model {
            id: "us.anthropic.claude-opus-4-1-20250805-v1:0".into(),
            name: "Claude Opus 4.1 (US)".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 15.0,
                output: 75.0,
                cache_read: 1.5,
                cache_write: 18.75,
            },
            context_window: 200000,
            max_tokens: 32000,
            headers: None,
            compat: None,
        },
        Model {
            id: "us.anthropic.claude-opus-4-20250514-v1:0".into(),
            name: "Claude Opus 4 (US)".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 15.0,
                output: 75.0,
                cache_read: 1.5,
                cache_write: 18.75,
            },
            context_window: 200000,
            max_tokens: 32000,
            headers: None,
            compat: None,
        },
        Model {
            id: "us.anthropic.claude-opus-4-5-20251101-v1:0".into(),
            name: "Claude Opus 4.5 (US)".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 5.0,
                output: 25.0,
                cache_read: 0.5,
                cache_write: 6.25,
            },
            context_window: 200000,
            max_tokens: 64000,
            headers: None,
            compat: None,
        },
        Model {
            id: "us.anthropic.claude-opus-4-6-v1".into(),
            name: "Claude Opus 4.6 (US)".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 5.0,
                output: 25.0,
                cache_read: 0.5,
                cache_write: 6.25,
            },
            context_window: 200000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "us.anthropic.claude-sonnet-4-20250514-v1:0".into(),
            name: "Claude Sonnet 4 (US)".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 3.0,
                output: 15.0,
                cache_read: 0.3,
                cache_write: 3.75,
            },
            context_window: 200000,
            max_tokens: 64000,
            headers: None,
            compat: None,
        },
        Model {
            id: "us.anthropic.claude-sonnet-4-5-20250929-v1:0".into(),
            name: "Claude Sonnet 4.5 (US)".into(),
            api: "bedrock-converse-stream".into(),
            provider: "amazon-bedrock".into(),
            base_url: "https://bedrock-runtime.us-east-1.amazonaws.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 3.0,
                output: 15.0,
                cache_read: 0.3,
                cache_write: 3.75,
            },
            context_window: 200000,
            max_tokens: 64000,
            headers: None,
            compat: None,
        },
        Model {
            id: "claude-3-5-haiku-20241022".into(),
            name: "Claude Haiku 3.5".into(),
            api: "anthropic-messages".into(),
            provider: "anthropic".into(),
            base_url: "https://api.anthropic.com".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.8,
                output: 4.0,
                cache_read: 0.08,
                cache_write: 1.0,
            },
            context_window: 200000,
            max_tokens: 8192,
            headers: None,
            compat: None,
        },
        Model {
            id: "claude-3-5-haiku-latest".into(),
            name: "Claude Haiku 3.5 (latest)".into(),
            api: "anthropic-messages".into(),
            provider: "anthropic".into(),
            base_url: "https://api.anthropic.com".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.8,
                output: 4.0,
                cache_read: 0.08,
                cache_write: 1.0,
            },
            context_window: 200000,
            max_tokens: 8192,
            headers: None,
            compat: None,
        },
        Model {
            id: "claude-3-5-sonnet-20240620".into(),
            name: "Claude Sonnet 3.5".into(),
            api: "anthropic-messages".into(),
            provider: "anthropic".into(),
            base_url: "https://api.anthropic.com".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 3.0,
                output: 15.0,
                cache_read: 0.3,
                cache_write: 3.75,
            },
            context_window: 200000,
            max_tokens: 8192,
            headers: None,
            compat: None,
        },
        Model {
            id: "claude-3-5-sonnet-20241022".into(),
            name: "Claude Sonnet 3.5 v2".into(),
            api: "anthropic-messages".into(),
            provider: "anthropic".into(),
            base_url: "https://api.anthropic.com".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 3.0,
                output: 15.0,
                cache_read: 0.3,
                cache_write: 3.75,
            },
            context_window: 200000,
            max_tokens: 8192,
            headers: None,
            compat: None,
        },
        Model {
            id: "claude-3-7-sonnet-20250219".into(),
            name: "Claude Sonnet 3.7".into(),
            api: "anthropic-messages".into(),
            provider: "anthropic".into(),
            base_url: "https://api.anthropic.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 3.0,
                output: 15.0,
                cache_read: 0.3,
                cache_write: 3.75,
            },
            context_window: 200000,
            max_tokens: 64000,
            headers: None,
            compat: None,
        },
        Model {
            id: "claude-3-7-sonnet-latest".into(),
            name: "Claude Sonnet 3.7 (latest)".into(),
            api: "anthropic-messages".into(),
            provider: "anthropic".into(),
            base_url: "https://api.anthropic.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 3.0,
                output: 15.0,
                cache_read: 0.3,
                cache_write: 3.75,
            },
            context_window: 200000,
            max_tokens: 64000,
            headers: None,
            compat: None,
        },
        Model {
            id: "claude-3-haiku-20240307".into(),
            name: "Claude Haiku 3".into(),
            api: "anthropic-messages".into(),
            provider: "anthropic".into(),
            base_url: "https://api.anthropic.com".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.25,
                output: 1.25,
                cache_read: 0.03,
                cache_write: 0.3,
            },
            context_window: 200000,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "claude-3-opus-20240229".into(),
            name: "Claude Opus 3".into(),
            api: "anthropic-messages".into(),
            provider: "anthropic".into(),
            base_url: "https://api.anthropic.com".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 15.0,
                output: 75.0,
                cache_read: 1.5,
                cache_write: 18.75,
            },
            context_window: 200000,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "claude-3-sonnet-20240229".into(),
            name: "Claude Sonnet 3".into(),
            api: "anthropic-messages".into(),
            provider: "anthropic".into(),
            base_url: "https://api.anthropic.com".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 3.0,
                output: 15.0,
                cache_read: 0.3,
                cache_write: 0.3,
            },
            context_window: 200000,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "claude-haiku-4-5".into(),
            name: "Claude Haiku 4.5 (latest)".into(),
            api: "anthropic-messages".into(),
            provider: "anthropic".into(),
            base_url: "https://api.anthropic.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.0,
                output: 5.0,
                cache_read: 0.1,
                cache_write: 1.25,
            },
            context_window: 200000,
            max_tokens: 64000,
            headers: None,
            compat: None,
        },
        Model {
            id: "claude-haiku-4-5-20251001".into(),
            name: "Claude Haiku 4.5".into(),
            api: "anthropic-messages".into(),
            provider: "anthropic".into(),
            base_url: "https://api.anthropic.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.0,
                output: 5.0,
                cache_read: 0.1,
                cache_write: 1.25,
            },
            context_window: 200000,
            max_tokens: 64000,
            headers: None,
            compat: None,
        },
        Model {
            id: "claude-opus-4-0".into(),
            name: "Claude Opus 4 (latest)".into(),
            api: "anthropic-messages".into(),
            provider: "anthropic".into(),
            base_url: "https://api.anthropic.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 15.0,
                output: 75.0,
                cache_read: 1.5,
                cache_write: 18.75,
            },
            context_window: 200000,
            max_tokens: 32000,
            headers: None,
            compat: None,
        },
        Model {
            id: "claude-opus-4-1".into(),
            name: "Claude Opus 4.1 (latest)".into(),
            api: "anthropic-messages".into(),
            provider: "anthropic".into(),
            base_url: "https://api.anthropic.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 15.0,
                output: 75.0,
                cache_read: 1.5,
                cache_write: 18.75,
            },
            context_window: 200000,
            max_tokens: 32000,
            headers: None,
            compat: None,
        },
        Model {
            id: "claude-opus-4-1-20250805".into(),
            name: "Claude Opus 4.1".into(),
            api: "anthropic-messages".into(),
            provider: "anthropic".into(),
            base_url: "https://api.anthropic.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 15.0,
                output: 75.0,
                cache_read: 1.5,
                cache_write: 18.75,
            },
            context_window: 200000,
            max_tokens: 32000,
            headers: None,
            compat: None,
        },
        Model {
            id: "claude-opus-4-20250514".into(),
            name: "Claude Opus 4".into(),
            api: "anthropic-messages".into(),
            provider: "anthropic".into(),
            base_url: "https://api.anthropic.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 15.0,
                output: 75.0,
                cache_read: 1.5,
                cache_write: 18.75,
            },
            context_window: 200000,
            max_tokens: 32000,
            headers: None,
            compat: None,
        },
        Model {
            id: "claude-opus-4-5".into(),
            name: "Claude Opus 4.5 (latest)".into(),
            api: "anthropic-messages".into(),
            provider: "anthropic".into(),
            base_url: "https://api.anthropic.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 5.0,
                output: 25.0,
                cache_read: 0.5,
                cache_write: 6.25,
            },
            context_window: 200000,
            max_tokens: 64000,
            headers: None,
            compat: None,
        },
        Model {
            id: "claude-opus-4-5-20251101".into(),
            name: "Claude Opus 4.5".into(),
            api: "anthropic-messages".into(),
            provider: "anthropic".into(),
            base_url: "https://api.anthropic.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 5.0,
                output: 25.0,
                cache_read: 0.5,
                cache_write: 6.25,
            },
            context_window: 200000,
            max_tokens: 64000,
            headers: None,
            compat: None,
        },
        Model {
            id: "claude-opus-4-6".into(),
            name: "Claude Opus 4.6".into(),
            api: "anthropic-messages".into(),
            provider: "anthropic".into(),
            base_url: "https://api.anthropic.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 5.0,
                output: 25.0,
                cache_read: 0.5,
                cache_write: 6.25,
            },
            context_window: 200000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "claude-sonnet-4-0".into(),
            name: "Claude Sonnet 4 (latest)".into(),
            api: "anthropic-messages".into(),
            provider: "anthropic".into(),
            base_url: "https://api.anthropic.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 3.0,
                output: 15.0,
                cache_read: 0.3,
                cache_write: 3.75,
            },
            context_window: 200000,
            max_tokens: 64000,
            headers: None,
            compat: None,
        },
        Model {
            id: "claude-sonnet-4-20250514".into(),
            name: "Claude Sonnet 4".into(),
            api: "anthropic-messages".into(),
            provider: "anthropic".into(),
            base_url: "https://api.anthropic.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 3.0,
                output: 15.0,
                cache_read: 0.3,
                cache_write: 3.75,
            },
            context_window: 200000,
            max_tokens: 64000,
            headers: None,
            compat: None,
        },
        Model {
            id: "claude-sonnet-4-5".into(),
            name: "Claude Sonnet 4.5 (latest)".into(),
            api: "anthropic-messages".into(),
            provider: "anthropic".into(),
            base_url: "https://api.anthropic.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 3.0,
                output: 15.0,
                cache_read: 0.3,
                cache_write: 3.75,
            },
            context_window: 200000,
            max_tokens: 64000,
            headers: None,
            compat: None,
        },
        Model {
            id: "claude-sonnet-4-5-20250929".into(),
            name: "Claude Sonnet 4.5".into(),
            api: "anthropic-messages".into(),
            provider: "anthropic".into(),
            base_url: "https://api.anthropic.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 3.0,
                output: 15.0,
                cache_read: 0.3,
                cache_write: 3.75,
            },
            context_window: 200000,
            max_tokens: 64000,
            headers: None,
            compat: None,
        },
        Model {
            id: "codex-mini-latest".into(),
            name: "Codex Mini".into(),
            api: "azure-openai-responses".into(),
            provider: "azure-openai-responses".into(),
            base_url: "".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 1.5,
                output: 6.0,
                cache_read: 0.375,
                cache_write: 0.0,
            },
            context_window: 200000,
            max_tokens: 100000,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-4".into(),
            name: "GPT-4".into(),
            api: "azure-openai-responses".into(),
            provider: "azure-openai-responses".into(),
            base_url: "".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 30.0,
                output: 60.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 8192,
            max_tokens: 8192,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-4-turbo".into(),
            name: "GPT-4 Turbo".into(),
            api: "azure-openai-responses".into(),
            provider: "azure-openai-responses".into(),
            base_url: "".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 10.0,
                output: 30.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-4.1".into(),
            name: "GPT-4.1".into(),
            api: "azure-openai-responses".into(),
            provider: "azure-openai-responses".into(),
            base_url: "".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 2.0,
                output: 8.0,
                cache_read: 0.5,
                cache_write: 0.0,
            },
            context_window: 1047576,
            max_tokens: 32768,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-4.1-mini".into(),
            name: "GPT-4.1 mini".into(),
            api: "azure-openai-responses".into(),
            provider: "azure-openai-responses".into(),
            base_url: "".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.4,
                output: 1.6,
                cache_read: 0.1,
                cache_write: 0.0,
            },
            context_window: 1047576,
            max_tokens: 32768,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-4.1-nano".into(),
            name: "GPT-4.1 nano".into(),
            api: "azure-openai-responses".into(),
            provider: "azure-openai-responses".into(),
            base_url: "".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.1,
                output: 0.4,
                cache_read: 0.03,
                cache_write: 0.0,
            },
            context_window: 1047576,
            max_tokens: 32768,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-4o".into(),
            name: "GPT-4o".into(),
            api: "azure-openai-responses".into(),
            provider: "azure-openai-responses".into(),
            base_url: "".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 2.5,
                output: 10.0,
                cache_read: 1.25,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 16384,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-4o-2024-05-13".into(),
            name: "GPT-4o (2024-05-13)".into(),
            api: "azure-openai-responses".into(),
            provider: "azure-openai-responses".into(),
            base_url: "".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 5.0,
                output: 15.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-4o-2024-08-06".into(),
            name: "GPT-4o (2024-08-06)".into(),
            api: "azure-openai-responses".into(),
            provider: "azure-openai-responses".into(),
            base_url: "".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 2.5,
                output: 10.0,
                cache_read: 1.25,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 16384,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-4o-2024-11-20".into(),
            name: "GPT-4o (2024-11-20)".into(),
            api: "azure-openai-responses".into(),
            provider: "azure-openai-responses".into(),
            base_url: "".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 2.5,
                output: 10.0,
                cache_read: 1.25,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 16384,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-4o-mini".into(),
            name: "GPT-4o mini".into(),
            api: "azure-openai-responses".into(),
            provider: "azure-openai-responses".into(),
            base_url: "".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.15,
                output: 0.6,
                cache_read: 0.08,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 16384,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-5".into(),
            name: "GPT-5".into(),
            api: "azure-openai-responses".into(),
            provider: "azure-openai-responses".into(),
            base_url: "".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.25,
                output: 10.0,
                cache_read: 0.125,
                cache_write: 0.0,
            },
            context_window: 400000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-5-chat-latest".into(),
            name: "GPT-5 Chat Latest".into(),
            api: "azure-openai-responses".into(),
            provider: "azure-openai-responses".into(),
            base_url: "".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.25,
                output: 10.0,
                cache_read: 0.125,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 16384,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-5-codex".into(),
            name: "GPT-5-Codex".into(),
            api: "azure-openai-responses".into(),
            provider: "azure-openai-responses".into(),
            base_url: "".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.25,
                output: 10.0,
                cache_read: 0.125,
                cache_write: 0.0,
            },
            context_window: 400000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-5-mini".into(),
            name: "GPT-5 Mini".into(),
            api: "azure-openai-responses".into(),
            provider: "azure-openai-responses".into(),
            base_url: "".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.25,
                output: 2.0,
                cache_read: 0.025,
                cache_write: 0.0,
            },
            context_window: 400000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-5-nano".into(),
            name: "GPT-5 Nano".into(),
            api: "azure-openai-responses".into(),
            provider: "azure-openai-responses".into(),
            base_url: "".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.05,
                output: 0.4,
                cache_read: 0.005,
                cache_write: 0.0,
            },
            context_window: 400000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-5-pro".into(),
            name: "GPT-5 Pro".into(),
            api: "azure-openai-responses".into(),
            provider: "azure-openai-responses".into(),
            base_url: "".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 15.0,
                output: 120.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 400000,
            max_tokens: 272000,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-5.1".into(),
            name: "GPT-5.1".into(),
            api: "azure-openai-responses".into(),
            provider: "azure-openai-responses".into(),
            base_url: "".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.25,
                output: 10.0,
                cache_read: 0.13,
                cache_write: 0.0,
            },
            context_window: 400000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-5.1-chat-latest".into(),
            name: "GPT-5.1 Chat".into(),
            api: "azure-openai-responses".into(),
            provider: "azure-openai-responses".into(),
            base_url: "".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.25,
                output: 10.0,
                cache_read: 0.125,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 16384,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-5.1-codex".into(),
            name: "GPT-5.1 Codex".into(),
            api: "azure-openai-responses".into(),
            provider: "azure-openai-responses".into(),
            base_url: "".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.25,
                output: 10.0,
                cache_read: 0.125,
                cache_write: 0.0,
            },
            context_window: 400000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-5.1-codex-max".into(),
            name: "GPT-5.1 Codex Max".into(),
            api: "azure-openai-responses".into(),
            provider: "azure-openai-responses".into(),
            base_url: "".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.25,
                output: 10.0,
                cache_read: 0.125,
                cache_write: 0.0,
            },
            context_window: 400000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-5.1-codex-mini".into(),
            name: "GPT-5.1 Codex mini".into(),
            api: "azure-openai-responses".into(),
            provider: "azure-openai-responses".into(),
            base_url: "".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.25,
                output: 2.0,
                cache_read: 0.025,
                cache_write: 0.0,
            },
            context_window: 400000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-5.2".into(),
            name: "GPT-5.2".into(),
            api: "azure-openai-responses".into(),
            provider: "azure-openai-responses".into(),
            base_url: "".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.75,
                output: 14.0,
                cache_read: 0.175,
                cache_write: 0.0,
            },
            context_window: 400000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-5.2-chat-latest".into(),
            name: "GPT-5.2 Chat".into(),
            api: "azure-openai-responses".into(),
            provider: "azure-openai-responses".into(),
            base_url: "".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.75,
                output: 14.0,
                cache_read: 0.175,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 16384,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-5.2-codex".into(),
            name: "GPT-5.2 Codex".into(),
            api: "azure-openai-responses".into(),
            provider: "azure-openai-responses".into(),
            base_url: "".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.75,
                output: 14.0,
                cache_read: 0.175,
                cache_write: 0.0,
            },
            context_window: 400000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-5.2-pro".into(),
            name: "GPT-5.2 Pro".into(),
            api: "azure-openai-responses".into(),
            provider: "azure-openai-responses".into(),
            base_url: "".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 21.0,
                output: 168.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 400000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-5.3-codex".into(),
            name: "GPT-5.3 Codex".into(),
            api: "azure-openai-responses".into(),
            provider: "azure-openai-responses".into(),
            base_url: "".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.75,
                output: 14.0,
                cache_read: 0.175,
                cache_write: 0.0,
            },
            context_window: 400000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "o1".into(),
            name: "o1".into(),
            api: "azure-openai-responses".into(),
            provider: "azure-openai-responses".into(),
            base_url: "".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 15.0,
                output: 60.0,
                cache_read: 7.5,
                cache_write: 0.0,
            },
            context_window: 200000,
            max_tokens: 100000,
            headers: None,
            compat: None,
        },
        Model {
            id: "o1-pro".into(),
            name: "o1-pro".into(),
            api: "azure-openai-responses".into(),
            provider: "azure-openai-responses".into(),
            base_url: "".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 150.0,
                output: 600.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 200000,
            max_tokens: 100000,
            headers: None,
            compat: None,
        },
        Model {
            id: "o3".into(),
            name: "o3".into(),
            api: "azure-openai-responses".into(),
            provider: "azure-openai-responses".into(),
            base_url: "".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 2.0,
                output: 8.0,
                cache_read: 0.5,
                cache_write: 0.0,
            },
            context_window: 200000,
            max_tokens: 100000,
            headers: None,
            compat: None,
        },
        Model {
            id: "o3-deep-research".into(),
            name: "o3-deep-research".into(),
            api: "azure-openai-responses".into(),
            provider: "azure-openai-responses".into(),
            base_url: "".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 10.0,
                output: 40.0,
                cache_read: 2.5,
                cache_write: 0.0,
            },
            context_window: 200000,
            max_tokens: 100000,
            headers: None,
            compat: None,
        },
        Model {
            id: "o3-mini".into(),
            name: "o3-mini".into(),
            api: "azure-openai-responses".into(),
            provider: "azure-openai-responses".into(),
            base_url: "".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 1.1,
                output: 4.4,
                cache_read: 0.55,
                cache_write: 0.0,
            },
            context_window: 200000,
            max_tokens: 100000,
            headers: None,
            compat: None,
        },
        Model {
            id: "o3-pro".into(),
            name: "o3-pro".into(),
            api: "azure-openai-responses".into(),
            provider: "azure-openai-responses".into(),
            base_url: "".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 20.0,
                output: 80.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 200000,
            max_tokens: 100000,
            headers: None,
            compat: None,
        },
        Model {
            id: "o4-mini".into(),
            name: "o4-mini".into(),
            api: "azure-openai-responses".into(),
            provider: "azure-openai-responses".into(),
            base_url: "".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.1,
                output: 4.4,
                cache_read: 0.28,
                cache_write: 0.0,
            },
            context_window: 200000,
            max_tokens: 100000,
            headers: None,
            compat: None,
        },
        Model {
            id: "o4-mini-deep-research".into(),
            name: "o4-mini-deep-research".into(),
            api: "azure-openai-responses".into(),
            provider: "azure-openai-responses".into(),
            base_url: "".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 2.0,
                output: 8.0,
                cache_read: 0.5,
                cache_write: 0.0,
            },
            context_window: 200000,
            max_tokens: 100000,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-oss-120b".into(),
            name: "GPT OSS 120B".into(),
            api: "openai-completions".into(),
            provider: "cerebras".into(),
            base_url: "https://api.cerebras.ai/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.25,
                output: 0.69,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 32768,
            headers: None,
            compat: None,
        },
        Model {
            id: "qwen-3-235b-a22b-instruct-2507".into(),
            name: "Qwen 3 235B Instruct".into(),
            api: "openai-completions".into(),
            provider: "cerebras".into(),
            base_url: "https://api.cerebras.ai/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.6,
                output: 1.2,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131000,
            max_tokens: 32000,
            headers: None,
            compat: None,
        },
        Model {
            id: "zai-glm-4.7".into(),
            name: "Z.AI GLM-4.7".into(),
            api: "openai-completions".into(),
            provider: "cerebras".into(),
            base_url: "https://api.cerebras.ai/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.0,
                output: 0.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 40000,
            headers: None,
            compat: None,
        },
        Model {
            id: "claude-haiku-4.5".into(),
            name: "Claude Haiku 4.5".into(),
            api: "openai-completions".into(),
            provider: "github-copilot".into(),
            base_url: "https://api.individual.githubcopilot.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.0,
                output: 0.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 16000,
            headers: Some(HashMap::from([
                ("User-Agent".into(), "GitHubCopilotChat/0.35.0".into()),
                ("Editor-Version".into(), "vscode/1.107.0".into()),
                ("Editor-Plugin-Version".into(), "copilot-chat/0.35.0".into()),
                ("Copilot-Integration-Id".into(), "vscode-chat".into()),
            ])),
            compat: Some(
                json!({"supportsStore":false,"supportsDeveloperRole":false,"supportsReasoningEffort":false}),
            ),
        },
        Model {
            id: "claude-opus-4.5".into(),
            name: "Claude Opus 4.5".into(),
            api: "openai-completions".into(),
            provider: "github-copilot".into(),
            base_url: "https://api.individual.githubcopilot.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.0,
                output: 0.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 16000,
            headers: Some(HashMap::from([
                ("User-Agent".into(), "GitHubCopilotChat/0.35.0".into()),
                ("Editor-Version".into(), "vscode/1.107.0".into()),
                ("Editor-Plugin-Version".into(), "copilot-chat/0.35.0".into()),
                ("Copilot-Integration-Id".into(), "vscode-chat".into()),
            ])),
            compat: Some(
                json!({"supportsStore":false,"supportsDeveloperRole":false,"supportsReasoningEffort":false}),
            ),
        },
        Model {
            id: "claude-opus-4.6".into(),
            name: "Claude Opus 4.6".into(),
            api: "openai-completions".into(),
            provider: "github-copilot".into(),
            base_url: "https://api.individual.githubcopilot.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.0,
                output: 0.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 64000,
            headers: Some(HashMap::from([
                ("User-Agent".into(), "GitHubCopilotChat/0.35.0".into()),
                ("Editor-Version".into(), "vscode/1.107.0".into()),
                ("Editor-Plugin-Version".into(), "copilot-chat/0.35.0".into()),
                ("Copilot-Integration-Id".into(), "vscode-chat".into()),
            ])),
            compat: Some(
                json!({"supportsStore":false,"supportsDeveloperRole":false,"supportsReasoningEffort":false}),
            ),
        },
        Model {
            id: "claude-sonnet-4".into(),
            name: "Claude Sonnet 4".into(),
            api: "openai-completions".into(),
            provider: "github-copilot".into(),
            base_url: "https://api.individual.githubcopilot.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.0,
                output: 0.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 16000,
            headers: Some(HashMap::from([
                ("User-Agent".into(), "GitHubCopilotChat/0.35.0".into()),
                ("Editor-Version".into(), "vscode/1.107.0".into()),
                ("Editor-Plugin-Version".into(), "copilot-chat/0.35.0".into()),
                ("Copilot-Integration-Id".into(), "vscode-chat".into()),
            ])),
            compat: Some(
                json!({"supportsStore":false,"supportsDeveloperRole":false,"supportsReasoningEffort":false}),
            ),
        },
        Model {
            id: "claude-sonnet-4.5".into(),
            name: "Claude Sonnet 4.5".into(),
            api: "openai-completions".into(),
            provider: "github-copilot".into(),
            base_url: "https://api.individual.githubcopilot.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.0,
                output: 0.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 16000,
            headers: Some(HashMap::from([
                ("User-Agent".into(), "GitHubCopilotChat/0.35.0".into()),
                ("Editor-Version".into(), "vscode/1.107.0".into()),
                ("Editor-Plugin-Version".into(), "copilot-chat/0.35.0".into()),
                ("Copilot-Integration-Id".into(), "vscode-chat".into()),
            ])),
            compat: Some(
                json!({"supportsStore":false,"supportsDeveloperRole":false,"supportsReasoningEffort":false}),
            ),
        },
        Model {
            id: "gemini-2.5-pro".into(),
            name: "Gemini 2.5 Pro".into(),
            api: "openai-completions".into(),
            provider: "github-copilot".into(),
            base_url: "https://api.individual.githubcopilot.com".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.0,
                output: 0.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 64000,
            headers: Some(HashMap::from([
                ("User-Agent".into(), "GitHubCopilotChat/0.35.0".into()),
                ("Editor-Version".into(), "vscode/1.107.0".into()),
                ("Editor-Plugin-Version".into(), "copilot-chat/0.35.0".into()),
                ("Copilot-Integration-Id".into(), "vscode-chat".into()),
            ])),
            compat: Some(
                json!({"supportsStore":false,"supportsDeveloperRole":false,"supportsReasoningEffort":false}),
            ),
        },
        Model {
            id: "gemini-3-flash-preview".into(),
            name: "Gemini 3 Flash".into(),
            api: "openai-completions".into(),
            provider: "github-copilot".into(),
            base_url: "https://api.individual.githubcopilot.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.0,
                output: 0.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 64000,
            headers: Some(HashMap::from([
                ("User-Agent".into(), "GitHubCopilotChat/0.35.0".into()),
                ("Editor-Version".into(), "vscode/1.107.0".into()),
                ("Editor-Plugin-Version".into(), "copilot-chat/0.35.0".into()),
                ("Copilot-Integration-Id".into(), "vscode-chat".into()),
            ])),
            compat: Some(
                json!({"supportsStore":false,"supportsDeveloperRole":false,"supportsReasoningEffort":false}),
            ),
        },
        Model {
            id: "gemini-3-pro-preview".into(),
            name: "Gemini 3 Pro Preview".into(),
            api: "openai-completions".into(),
            provider: "github-copilot".into(),
            base_url: "https://api.individual.githubcopilot.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.0,
                output: 0.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 64000,
            headers: Some(HashMap::from([
                ("User-Agent".into(), "GitHubCopilotChat/0.35.0".into()),
                ("Editor-Version".into(), "vscode/1.107.0".into()),
                ("Editor-Plugin-Version".into(), "copilot-chat/0.35.0".into()),
                ("Copilot-Integration-Id".into(), "vscode-chat".into()),
            ])),
            compat: Some(
                json!({"supportsStore":false,"supportsDeveloperRole":false,"supportsReasoningEffort":false}),
            ),
        },
        Model {
            id: "gpt-4.1".into(),
            name: "GPT-4.1".into(),
            api: "openai-completions".into(),
            provider: "github-copilot".into(),
            base_url: "https://api.individual.githubcopilot.com".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.0,
                output: 0.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 16384,
            headers: Some(HashMap::from([
                ("User-Agent".into(), "GitHubCopilotChat/0.35.0".into()),
                ("Editor-Version".into(), "vscode/1.107.0".into()),
                ("Editor-Plugin-Version".into(), "copilot-chat/0.35.0".into()),
                ("Copilot-Integration-Id".into(), "vscode-chat".into()),
            ])),
            compat: Some(
                json!({"supportsStore":false,"supportsDeveloperRole":false,"supportsReasoningEffort":false}),
            ),
        },
        Model {
            id: "gpt-4o".into(),
            name: "GPT-4o".into(),
            api: "openai-completions".into(),
            provider: "github-copilot".into(),
            base_url: "https://api.individual.githubcopilot.com".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.0,
                output: 0.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 64000,
            max_tokens: 16384,
            headers: Some(HashMap::from([
                ("User-Agent".into(), "GitHubCopilotChat/0.35.0".into()),
                ("Editor-Version".into(), "vscode/1.107.0".into()),
                ("Editor-Plugin-Version".into(), "copilot-chat/0.35.0".into()),
                ("Copilot-Integration-Id".into(), "vscode-chat".into()),
            ])),
            compat: Some(
                json!({"supportsStore":false,"supportsDeveloperRole":false,"supportsReasoningEffort":false}),
            ),
        },
        Model {
            id: "gpt-5".into(),
            name: "GPT-5".into(),
            api: "openai-responses".into(),
            provider: "github-copilot".into(),
            base_url: "https://api.individual.githubcopilot.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.0,
                output: 0.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 128000,
            headers: Some(HashMap::from([
                ("User-Agent".into(), "GitHubCopilotChat/0.35.0".into()),
                ("Editor-Version".into(), "vscode/1.107.0".into()),
                ("Editor-Plugin-Version".into(), "copilot-chat/0.35.0".into()),
                ("Copilot-Integration-Id".into(), "vscode-chat".into()),
            ])),
            compat: None,
        },
        Model {
            id: "gpt-5-mini".into(),
            name: "GPT-5-mini".into(),
            api: "openai-responses".into(),
            provider: "github-copilot".into(),
            base_url: "https://api.individual.githubcopilot.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.0,
                output: 0.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 64000,
            headers: Some(HashMap::from([
                ("User-Agent".into(), "GitHubCopilotChat/0.35.0".into()),
                ("Editor-Version".into(), "vscode/1.107.0".into()),
                ("Editor-Plugin-Version".into(), "copilot-chat/0.35.0".into()),
                ("Copilot-Integration-Id".into(), "vscode-chat".into()),
            ])),
            compat: None,
        },
        Model {
            id: "gpt-5.1".into(),
            name: "GPT-5.1".into(),
            api: "openai-responses".into(),
            provider: "github-copilot".into(),
            base_url: "https://api.individual.githubcopilot.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.0,
                output: 0.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 128000,
            headers: Some(HashMap::from([
                ("User-Agent".into(), "GitHubCopilotChat/0.35.0".into()),
                ("Editor-Version".into(), "vscode/1.107.0".into()),
                ("Editor-Plugin-Version".into(), "copilot-chat/0.35.0".into()),
                ("Copilot-Integration-Id".into(), "vscode-chat".into()),
            ])),
            compat: None,
        },
        Model {
            id: "gpt-5.1-codex".into(),
            name: "GPT-5.1-Codex".into(),
            api: "openai-responses".into(),
            provider: "github-copilot".into(),
            base_url: "https://api.individual.githubcopilot.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.0,
                output: 0.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 128000,
            headers: Some(HashMap::from([
                ("User-Agent".into(), "GitHubCopilotChat/0.35.0".into()),
                ("Editor-Version".into(), "vscode/1.107.0".into()),
                ("Editor-Plugin-Version".into(), "copilot-chat/0.35.0".into()),
                ("Copilot-Integration-Id".into(), "vscode-chat".into()),
            ])),
            compat: None,
        },
        Model {
            id: "gpt-5.1-codex-max".into(),
            name: "GPT-5.1-Codex-max".into(),
            api: "openai-responses".into(),
            provider: "github-copilot".into(),
            base_url: "https://api.individual.githubcopilot.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.0,
                output: 0.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 128000,
            headers: Some(HashMap::from([
                ("User-Agent".into(), "GitHubCopilotChat/0.35.0".into()),
                ("Editor-Version".into(), "vscode/1.107.0".into()),
                ("Editor-Plugin-Version".into(), "copilot-chat/0.35.0".into()),
                ("Copilot-Integration-Id".into(), "vscode-chat".into()),
            ])),
            compat: None,
        },
        Model {
            id: "gpt-5.1-codex-mini".into(),
            name: "GPT-5.1-Codex-mini".into(),
            api: "openai-responses".into(),
            provider: "github-copilot".into(),
            base_url: "https://api.individual.githubcopilot.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.0,
                output: 0.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 100000,
            headers: Some(HashMap::from([
                ("User-Agent".into(), "GitHubCopilotChat/0.35.0".into()),
                ("Editor-Version".into(), "vscode/1.107.0".into()),
                ("Editor-Plugin-Version".into(), "copilot-chat/0.35.0".into()),
                ("Copilot-Integration-Id".into(), "vscode-chat".into()),
            ])),
            compat: None,
        },
        Model {
            id: "gpt-5.2".into(),
            name: "GPT-5.2".into(),
            api: "openai-responses".into(),
            provider: "github-copilot".into(),
            base_url: "https://api.individual.githubcopilot.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.0,
                output: 0.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 64000,
            headers: Some(HashMap::from([
                ("User-Agent".into(), "GitHubCopilotChat/0.35.0".into()),
                ("Editor-Version".into(), "vscode/1.107.0".into()),
                ("Editor-Plugin-Version".into(), "copilot-chat/0.35.0".into()),
                ("Copilot-Integration-Id".into(), "vscode-chat".into()),
            ])),
            compat: None,
        },
        Model {
            id: "gpt-5.2-codex".into(),
            name: "GPT-5.2-Codex".into(),
            api: "openai-responses".into(),
            provider: "github-copilot".into(),
            base_url: "https://api.individual.githubcopilot.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.0,
                output: 0.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 272000,
            max_tokens: 128000,
            headers: Some(HashMap::from([
                ("User-Agent".into(), "GitHubCopilotChat/0.35.0".into()),
                ("Editor-Version".into(), "vscode/1.107.0".into()),
                ("Editor-Plugin-Version".into(), "copilot-chat/0.35.0".into()),
                ("Copilot-Integration-Id".into(), "vscode-chat".into()),
            ])),
            compat: None,
        },
        Model {
            id: "grok-code-fast-1".into(),
            name: "Grok Code Fast 1".into(),
            api: "openai-completions".into(),
            provider: "github-copilot".into(),
            base_url: "https://api.individual.githubcopilot.com".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.0,
                output: 0.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 64000,
            headers: Some(HashMap::from([
                ("User-Agent".into(), "GitHubCopilotChat/0.35.0".into()),
                ("Editor-Version".into(), "vscode/1.107.0".into()),
                ("Editor-Plugin-Version".into(), "copilot-chat/0.35.0".into()),
                ("Copilot-Integration-Id".into(), "vscode-chat".into()),
            ])),
            compat: Some(
                json!({"supportsStore":false,"supportsDeveloperRole":false,"supportsReasoningEffort":false}),
            ),
        },
        Model {
            id: "gemini-1.5-flash".into(),
            name: "Gemini 1.5 Flash".into(),
            api: "google-generative-ai".into(),
            provider: "google".into(),
            base_url: "https://generativelanguage.googleapis.com/v1beta".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.075,
                output: 0.3,
                cache_read: 0.01875,
                cache_write: 0.0,
            },
            context_window: 1000000,
            max_tokens: 8192,
            headers: None,
            compat: None,
        },
        Model {
            id: "gemini-1.5-flash-8b".into(),
            name: "Gemini 1.5 Flash-8B".into(),
            api: "google-generative-ai".into(),
            provider: "google".into(),
            base_url: "https://generativelanguage.googleapis.com/v1beta".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.0375,
                output: 0.15,
                cache_read: 0.01,
                cache_write: 0.0,
            },
            context_window: 1000000,
            max_tokens: 8192,
            headers: None,
            compat: None,
        },
        Model {
            id: "gemini-1.5-pro".into(),
            name: "Gemini 1.5 Pro".into(),
            api: "google-generative-ai".into(),
            provider: "google".into(),
            base_url: "https://generativelanguage.googleapis.com/v1beta".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.25,
                output: 5.0,
                cache_read: 0.3125,
                cache_write: 0.0,
            },
            context_window: 1000000,
            max_tokens: 8192,
            headers: None,
            compat: None,
        },
        Model {
            id: "gemini-2.0-flash".into(),
            name: "Gemini 2.0 Flash".into(),
            api: "google-generative-ai".into(),
            provider: "google".into(),
            base_url: "https://generativelanguage.googleapis.com/v1beta".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.1,
                output: 0.4,
                cache_read: 0.025,
                cache_write: 0.0,
            },
            context_window: 1048576,
            max_tokens: 8192,
            headers: None,
            compat: None,
        },
        Model {
            id: "gemini-2.0-flash-lite".into(),
            name: "Gemini 2.0 Flash Lite".into(),
            api: "google-generative-ai".into(),
            provider: "google".into(),
            base_url: "https://generativelanguage.googleapis.com/v1beta".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.075,
                output: 0.3,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 1048576,
            max_tokens: 8192,
            headers: None,
            compat: None,
        },
        Model {
            id: "gemini-2.5-flash".into(),
            name: "Gemini 2.5 Flash".into(),
            api: "google-generative-ai".into(),
            provider: "google".into(),
            base_url: "https://generativelanguage.googleapis.com/v1beta".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.3,
                output: 2.5,
                cache_read: 0.075,
                cache_write: 0.0,
            },
            context_window: 1048576,
            max_tokens: 65536,
            headers: None,
            compat: None,
        },
        Model {
            id: "gemini-2.5-flash-lite".into(),
            name: "Gemini 2.5 Flash Lite".into(),
            api: "google-generative-ai".into(),
            provider: "google".into(),
            base_url: "https://generativelanguage.googleapis.com/v1beta".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.1,
                output: 0.4,
                cache_read: 0.025,
                cache_write: 0.0,
            },
            context_window: 1048576,
            max_tokens: 65536,
            headers: None,
            compat: None,
        },
        Model {
            id: "gemini-2.5-flash-lite-preview-06-17".into(),
            name: "Gemini 2.5 Flash Lite Preview 06-17".into(),
            api: "google-generative-ai".into(),
            provider: "google".into(),
            base_url: "https://generativelanguage.googleapis.com/v1beta".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.1,
                output: 0.4,
                cache_read: 0.025,
                cache_write: 0.0,
            },
            context_window: 1048576,
            max_tokens: 65536,
            headers: None,
            compat: None,
        },
        Model {
            id: "gemini-2.5-flash-lite-preview-09-2025".into(),
            name: "Gemini 2.5 Flash Lite Preview 09-25".into(),
            api: "google-generative-ai".into(),
            provider: "google".into(),
            base_url: "https://generativelanguage.googleapis.com/v1beta".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.1,
                output: 0.4,
                cache_read: 0.025,
                cache_write: 0.0,
            },
            context_window: 1048576,
            max_tokens: 65536,
            headers: None,
            compat: None,
        },
        Model {
            id: "gemini-2.5-flash-preview-04-17".into(),
            name: "Gemini 2.5 Flash Preview 04-17".into(),
            api: "google-generative-ai".into(),
            provider: "google".into(),
            base_url: "https://generativelanguage.googleapis.com/v1beta".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.15,
                output: 0.6,
                cache_read: 0.0375,
                cache_write: 0.0,
            },
            context_window: 1048576,
            max_tokens: 65536,
            headers: None,
            compat: None,
        },
        Model {
            id: "gemini-2.5-flash-preview-05-20".into(),
            name: "Gemini 2.5 Flash Preview 05-20".into(),
            api: "google-generative-ai".into(),
            provider: "google".into(),
            base_url: "https://generativelanguage.googleapis.com/v1beta".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.15,
                output: 0.6,
                cache_read: 0.0375,
                cache_write: 0.0,
            },
            context_window: 1048576,
            max_tokens: 65536,
            headers: None,
            compat: None,
        },
        Model {
            id: "gemini-2.5-flash-preview-09-2025".into(),
            name: "Gemini 2.5 Flash Preview 09-25".into(),
            api: "google-generative-ai".into(),
            provider: "google".into(),
            base_url: "https://generativelanguage.googleapis.com/v1beta".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.3,
                output: 2.5,
                cache_read: 0.075,
                cache_write: 0.0,
            },
            context_window: 1048576,
            max_tokens: 65536,
            headers: None,
            compat: None,
        },
        Model {
            id: "gemini-2.5-pro".into(),
            name: "Gemini 2.5 Pro".into(),
            api: "google-generative-ai".into(),
            provider: "google".into(),
            base_url: "https://generativelanguage.googleapis.com/v1beta".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.25,
                output: 10.0,
                cache_read: 0.31,
                cache_write: 0.0,
            },
            context_window: 1048576,
            max_tokens: 65536,
            headers: None,
            compat: None,
        },
        Model {
            id: "gemini-2.5-pro-preview-05-06".into(),
            name: "Gemini 2.5 Pro Preview 05-06".into(),
            api: "google-generative-ai".into(),
            provider: "google".into(),
            base_url: "https://generativelanguage.googleapis.com/v1beta".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.25,
                output: 10.0,
                cache_read: 0.31,
                cache_write: 0.0,
            },
            context_window: 1048576,
            max_tokens: 65536,
            headers: None,
            compat: None,
        },
        Model {
            id: "gemini-2.5-pro-preview-06-05".into(),
            name: "Gemini 2.5 Pro Preview 06-05".into(),
            api: "google-generative-ai".into(),
            provider: "google".into(),
            base_url: "https://generativelanguage.googleapis.com/v1beta".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.25,
                output: 10.0,
                cache_read: 0.31,
                cache_write: 0.0,
            },
            context_window: 1048576,
            max_tokens: 65536,
            headers: None,
            compat: None,
        },
        Model {
            id: "gemini-3-flash-preview".into(),
            name: "Gemini 3 Flash Preview".into(),
            api: "google-generative-ai".into(),
            provider: "google".into(),
            base_url: "https://generativelanguage.googleapis.com/v1beta".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.5,
                output: 3.0,
                cache_read: 0.05,
                cache_write: 0.0,
            },
            context_window: 1048576,
            max_tokens: 65536,
            headers: None,
            compat: None,
        },
        Model {
            id: "gemini-3-pro-preview".into(),
            name: "Gemini 3 Pro Preview".into(),
            api: "google-generative-ai".into(),
            provider: "google".into(),
            base_url: "https://generativelanguage.googleapis.com/v1beta".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 2.0,
                output: 12.0,
                cache_read: 0.2,
                cache_write: 0.0,
            },
            context_window: 1000000,
            max_tokens: 64000,
            headers: None,
            compat: None,
        },
        Model {
            id: "gemini-flash-latest".into(),
            name: "Gemini Flash Latest".into(),
            api: "google-generative-ai".into(),
            provider: "google".into(),
            base_url: "https://generativelanguage.googleapis.com/v1beta".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.3,
                output: 2.5,
                cache_read: 0.075,
                cache_write: 0.0,
            },
            context_window: 1048576,
            max_tokens: 65536,
            headers: None,
            compat: None,
        },
        Model {
            id: "gemini-flash-lite-latest".into(),
            name: "Gemini Flash-Lite Latest".into(),
            api: "google-generative-ai".into(),
            provider: "google".into(),
            base_url: "https://generativelanguage.googleapis.com/v1beta".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.1,
                output: 0.4,
                cache_read: 0.025,
                cache_write: 0.0,
            },
            context_window: 1048576,
            max_tokens: 65536,
            headers: None,
            compat: None,
        },
        Model {
            id: "gemini-live-2.5-flash".into(),
            name: "Gemini Live 2.5 Flash".into(),
            api: "google-generative-ai".into(),
            provider: "google".into(),
            base_url: "https://generativelanguage.googleapis.com/v1beta".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.5,
                output: 2.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 8000,
            headers: None,
            compat: None,
        },
        Model {
            id: "gemini-live-2.5-flash-preview-native-audio".into(),
            name: "Gemini Live 2.5 Flash Preview Native Audio".into(),
            api: "google-generative-ai".into(),
            provider: "google".into(),
            base_url: "https://generativelanguage.googleapis.com/v1beta".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.5,
                output: 2.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 65536,
            headers: None,
            compat: None,
        },
        Model {
            id: "claude-opus-4-5-thinking".into(),
            name: "Claude Opus 4.5 Thinking (Antigravity)".into(),
            api: "google-gemini-cli".into(),
            provider: "google-antigravity".into(),
            base_url: "https://daily-cloudcode-pa.sandbox.googleapis.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 5.0,
                output: 25.0,
                cache_read: 0.5,
                cache_write: 6.25,
            },
            context_window: 200000,
            max_tokens: 64000,
            headers: None,
            compat: None,
        },
        Model {
            id: "claude-sonnet-4-5".into(),
            name: "Claude Sonnet 4.5 (Antigravity)".into(),
            api: "google-gemini-cli".into(),
            provider: "google-antigravity".into(),
            base_url: "https://daily-cloudcode-pa.sandbox.googleapis.com".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 3.0,
                output: 15.0,
                cache_read: 0.3,
                cache_write: 3.75,
            },
            context_window: 200000,
            max_tokens: 64000,
            headers: None,
            compat: None,
        },
        Model {
            id: "claude-sonnet-4-5-thinking".into(),
            name: "Claude Sonnet 4.5 Thinking (Antigravity)".into(),
            api: "google-gemini-cli".into(),
            provider: "google-antigravity".into(),
            base_url: "https://daily-cloudcode-pa.sandbox.googleapis.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 3.0,
                output: 15.0,
                cache_read: 0.3,
                cache_write: 3.75,
            },
            context_window: 200000,
            max_tokens: 64000,
            headers: None,
            compat: None,
        },
        Model {
            id: "gemini-3-flash".into(),
            name: "Gemini 3 Flash (Antigravity)".into(),
            api: "google-gemini-cli".into(),
            provider: "google-antigravity".into(),
            base_url: "https://daily-cloudcode-pa.sandbox.googleapis.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.5,
                output: 3.0,
                cache_read: 0.5,
                cache_write: 0.0,
            },
            context_window: 1048576,
            max_tokens: 65535,
            headers: None,
            compat: None,
        },
        Model {
            id: "gemini-3-pro-high".into(),
            name: "Gemini 3 Pro High (Antigravity)".into(),
            api: "google-gemini-cli".into(),
            provider: "google-antigravity".into(),
            base_url: "https://daily-cloudcode-pa.sandbox.googleapis.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 2.0,
                output: 12.0,
                cache_read: 0.2,
                cache_write: 2.375,
            },
            context_window: 1048576,
            max_tokens: 65535,
            headers: None,
            compat: None,
        },
        Model {
            id: "gemini-3-pro-low".into(),
            name: "Gemini 3 Pro Low (Antigravity)".into(),
            api: "google-gemini-cli".into(),
            provider: "google-antigravity".into(),
            base_url: "https://daily-cloudcode-pa.sandbox.googleapis.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 2.0,
                output: 12.0,
                cache_read: 0.2,
                cache_write: 2.375,
            },
            context_window: 1048576,
            max_tokens: 65535,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-oss-120b-medium".into(),
            name: "GPT-OSS 120B Medium (Antigravity)".into(),
            api: "google-gemini-cli".into(),
            provider: "google-antigravity".into(),
            base_url: "https://daily-cloudcode-pa.sandbox.googleapis.com".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.09,
                output: 0.36,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 32768,
            headers: None,
            compat: None,
        },
        Model {
            id: "gemini-2.0-flash".into(),
            name: "Gemini 2.0 Flash (Cloud Code Assist)".into(),
            api: "google-gemini-cli".into(),
            provider: "google-gemini-cli".into(),
            base_url: "https://cloudcode-pa.googleapis.com".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.0,
                output: 0.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 1048576,
            max_tokens: 8192,
            headers: None,
            compat: None,
        },
        Model {
            id: "gemini-2.5-flash".into(),
            name: "Gemini 2.5 Flash (Cloud Code Assist)".into(),
            api: "google-gemini-cli".into(),
            provider: "google-gemini-cli".into(),
            base_url: "https://cloudcode-pa.googleapis.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.0,
                output: 0.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 1048576,
            max_tokens: 65535,
            headers: None,
            compat: None,
        },
        Model {
            id: "gemini-2.5-pro".into(),
            name: "Gemini 2.5 Pro (Cloud Code Assist)".into(),
            api: "google-gemini-cli".into(),
            provider: "google-gemini-cli".into(),
            base_url: "https://cloudcode-pa.googleapis.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.0,
                output: 0.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 1048576,
            max_tokens: 65535,
            headers: None,
            compat: None,
        },
        Model {
            id: "gemini-3-flash-preview".into(),
            name: "Gemini 3 Flash Preview (Cloud Code Assist)".into(),
            api: "google-gemini-cli".into(),
            provider: "google-gemini-cli".into(),
            base_url: "https://cloudcode-pa.googleapis.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.0,
                output: 0.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 1048576,
            max_tokens: 65535,
            headers: None,
            compat: None,
        },
        Model {
            id: "gemini-3-pro-preview".into(),
            name: "Gemini 3 Pro Preview (Cloud Code Assist)".into(),
            api: "google-gemini-cli".into(),
            provider: "google-gemini-cli".into(),
            base_url: "https://cloudcode-pa.googleapis.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.0,
                output: 0.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 1048576,
            max_tokens: 65535,
            headers: None,
            compat: None,
        },
        Model {
            id: "gemini-1.5-flash".into(),
            name: "Gemini 1.5 Flash (Vertex)".into(),
            api: "google-vertex".into(),
            provider: "google-vertex".into(),
            base_url: "https://{location}-aiplatform.googleapis.com".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.075,
                output: 0.3,
                cache_read: 0.01875,
                cache_write: 0.0,
            },
            context_window: 1000000,
            max_tokens: 8192,
            headers: None,
            compat: None,
        },
        Model {
            id: "gemini-1.5-flash-8b".into(),
            name: "Gemini 1.5 Flash-8B (Vertex)".into(),
            api: "google-vertex".into(),
            provider: "google-vertex".into(),
            base_url: "https://{location}-aiplatform.googleapis.com".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.0375,
                output: 0.15,
                cache_read: 0.01,
                cache_write: 0.0,
            },
            context_window: 1000000,
            max_tokens: 8192,
            headers: None,
            compat: None,
        },
        Model {
            id: "gemini-1.5-pro".into(),
            name: "Gemini 1.5 Pro (Vertex)".into(),
            api: "google-vertex".into(),
            provider: "google-vertex".into(),
            base_url: "https://{location}-aiplatform.googleapis.com".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.25,
                output: 5.0,
                cache_read: 0.3125,
                cache_write: 0.0,
            },
            context_window: 1000000,
            max_tokens: 8192,
            headers: None,
            compat: None,
        },
        Model {
            id: "gemini-2.0-flash".into(),
            name: "Gemini 2.0 Flash (Vertex)".into(),
            api: "google-vertex".into(),
            provider: "google-vertex".into(),
            base_url: "https://{location}-aiplatform.googleapis.com".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.15,
                output: 0.6,
                cache_read: 0.0375,
                cache_write: 0.0,
            },
            context_window: 1048576,
            max_tokens: 8192,
            headers: None,
            compat: None,
        },
        Model {
            id: "gemini-2.0-flash-lite".into(),
            name: "Gemini 2.0 Flash Lite (Vertex)".into(),
            api: "google-vertex".into(),
            provider: "google-vertex".into(),
            base_url: "https://{location}-aiplatform.googleapis.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.075,
                output: 0.3,
                cache_read: 0.01875,
                cache_write: 0.0,
            },
            context_window: 1048576,
            max_tokens: 65536,
            headers: None,
            compat: None,
        },
        Model {
            id: "gemini-2.5-flash".into(),
            name: "Gemini 2.5 Flash (Vertex)".into(),
            api: "google-vertex".into(),
            provider: "google-vertex".into(),
            base_url: "https://{location}-aiplatform.googleapis.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.3,
                output: 2.5,
                cache_read: 0.03,
                cache_write: 0.0,
            },
            context_window: 1048576,
            max_tokens: 65536,
            headers: None,
            compat: None,
        },
        Model {
            id: "gemini-2.5-flash-lite".into(),
            name: "Gemini 2.5 Flash Lite (Vertex)".into(),
            api: "google-vertex".into(),
            provider: "google-vertex".into(),
            base_url: "https://{location}-aiplatform.googleapis.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.1,
                output: 0.4,
                cache_read: 0.01,
                cache_write: 0.0,
            },
            context_window: 1048576,
            max_tokens: 65536,
            headers: None,
            compat: None,
        },
        Model {
            id: "gemini-2.5-flash-lite-preview-09-2025".into(),
            name: "Gemini 2.5 Flash Lite Preview 09-25 (Vertex)".into(),
            api: "google-vertex".into(),
            provider: "google-vertex".into(),
            base_url: "https://{location}-aiplatform.googleapis.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.1,
                output: 0.4,
                cache_read: 0.01,
                cache_write: 0.0,
            },
            context_window: 1048576,
            max_tokens: 65536,
            headers: None,
            compat: None,
        },
        Model {
            id: "gemini-2.5-pro".into(),
            name: "Gemini 2.5 Pro (Vertex)".into(),
            api: "google-vertex".into(),
            provider: "google-vertex".into(),
            base_url: "https://{location}-aiplatform.googleapis.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.25,
                output: 10.0,
                cache_read: 0.125,
                cache_write: 0.0,
            },
            context_window: 1048576,
            max_tokens: 65536,
            headers: None,
            compat: None,
        },
        Model {
            id: "gemini-3-flash-preview".into(),
            name: "Gemini 3 Flash Preview (Vertex)".into(),
            api: "google-vertex".into(),
            provider: "google-vertex".into(),
            base_url: "https://{location}-aiplatform.googleapis.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.5,
                output: 3.0,
                cache_read: 0.05,
                cache_write: 0.0,
            },
            context_window: 1048576,
            max_tokens: 65536,
            headers: None,
            compat: None,
        },
        Model {
            id: "gemini-3-pro-preview".into(),
            name: "Gemini 3 Pro Preview (Vertex)".into(),
            api: "google-vertex".into(),
            provider: "google-vertex".into(),
            base_url: "https://{location}-aiplatform.googleapis.com".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 2.0,
                output: 12.0,
                cache_read: 0.2,
                cache_write: 0.0,
            },
            context_window: 1000000,
            max_tokens: 64000,
            headers: None,
            compat: None,
        },
        Model {
            id: "deepseek-r1-distill-llama-70b".into(),
            name: "DeepSeek R1 Distill Llama 70B".into(),
            api: "openai-completions".into(),
            provider: "groq".into(),
            base_url: "https://api.groq.com/openai/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.75,
                output: 0.99,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 8192,
            headers: None,
            compat: None,
        },
        Model {
            id: "gemma2-9b-it".into(),
            name: "Gemma 2 9B".into(),
            api: "openai-completions".into(),
            provider: "groq".into(),
            base_url: "https://api.groq.com/openai/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.2,
                output: 0.2,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 8192,
            max_tokens: 8192,
            headers: None,
            compat: None,
        },
        Model {
            id: "llama-3.1-8b-instant".into(),
            name: "Llama 3.1 8B Instant".into(),
            api: "openai-completions".into(),
            provider: "groq".into(),
            base_url: "https://api.groq.com/openai/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.05,
                output: 0.08,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 131072,
            headers: None,
            compat: None,
        },
        Model {
            id: "llama-3.3-70b-versatile".into(),
            name: "Llama 3.3 70B Versatile".into(),
            api: "openai-completions".into(),
            provider: "groq".into(),
            base_url: "https://api.groq.com/openai/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.59,
                output: 0.79,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 32768,
            headers: None,
            compat: None,
        },
        Model {
            id: "llama3-70b-8192".into(),
            name: "Llama 3 70B".into(),
            api: "openai-completions".into(),
            provider: "groq".into(),
            base_url: "https://api.groq.com/openai/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.59,
                output: 0.79,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 8192,
            max_tokens: 8192,
            headers: None,
            compat: None,
        },
        Model {
            id: "llama3-8b-8192".into(),
            name: "Llama 3 8B".into(),
            api: "openai-completions".into(),
            provider: "groq".into(),
            base_url: "https://api.groq.com/openai/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.05,
                output: 0.08,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 8192,
            max_tokens: 8192,
            headers: None,
            compat: None,
        },
        Model {
            id: "meta-llama/llama-4-maverick-17b-128e-instruct".into(),
            name: "Llama 4 Maverick 17B".into(),
            api: "openai-completions".into(),
            provider: "groq".into(),
            base_url: "https://api.groq.com/openai/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.2,
                output: 0.6,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 8192,
            headers: None,
            compat: None,
        },
        Model {
            id: "meta-llama/llama-4-scout-17b-16e-instruct".into(),
            name: "Llama 4 Scout 17B".into(),
            api: "openai-completions".into(),
            provider: "groq".into(),
            base_url: "https://api.groq.com/openai/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.11,
                output: 0.34,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 8192,
            headers: None,
            compat: None,
        },
        Model {
            id: "mistral-saba-24b".into(),
            name: "Mistral Saba 24B".into(),
            api: "openai-completions".into(),
            provider: "groq".into(),
            base_url: "https://api.groq.com/openai/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.79,
                output: 0.79,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 32768,
            max_tokens: 32768,
            headers: None,
            compat: None,
        },
        Model {
            id: "moonshotai/kimi-k2-instruct".into(),
            name: "Kimi K2 Instruct".into(),
            api: "openai-completions".into(),
            provider: "groq".into(),
            base_url: "https://api.groq.com/openai/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 1.0,
                output: 3.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 16384,
            headers: None,
            compat: None,
        },
        Model {
            id: "moonshotai/kimi-k2-instruct-0905".into(),
            name: "Kimi K2 Instruct 0905".into(),
            api: "openai-completions".into(),
            provider: "groq".into(),
            base_url: "https://api.groq.com/openai/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 1.0,
                output: 3.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 262144,
            max_tokens: 16384,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/gpt-oss-120b".into(),
            name: "GPT OSS 120B".into(),
            api: "openai-completions".into(),
            provider: "groq".into(),
            base_url: "https://api.groq.com/openai/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.15,
                output: 0.6,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 65536,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/gpt-oss-20b".into(),
            name: "GPT OSS 20B".into(),
            api: "openai-completions".into(),
            provider: "groq".into(),
            base_url: "https://api.groq.com/openai/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.075,
                output: 0.3,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 65536,
            headers: None,
            compat: None,
        },
        Model {
            id: "qwen-qwq-32b".into(),
            name: "Qwen QwQ 32B".into(),
            api: "openai-completions".into(),
            provider: "groq".into(),
            base_url: "https://api.groq.com/openai/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.29,
                output: 0.39,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 16384,
            headers: None,
            compat: None,
        },
        Model {
            id: "qwen/qwen3-32b".into(),
            name: "Qwen3 32B".into(),
            api: "openai-completions".into(),
            provider: "groq".into(),
            base_url: "https://api.groq.com/openai/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.29,
                output: 0.59,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 16384,
            headers: None,
            compat: None,
        },
        Model {
            id: "MiniMaxAI/MiniMax-M2.1".into(),
            name: "MiniMax-M2.1".into(),
            api: "openai-completions".into(),
            provider: "huggingface".into(),
            base_url: "https://router.huggingface.co/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.3,
                output: 1.2,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 204800,
            max_tokens: 131072,
            headers: None,
            compat: Some(json!({"supportsDeveloperRole":false})),
        },
        Model {
            id: "Qwen/Qwen3-235B-A22B-Thinking-2507".into(),
            name: "Qwen3-235B-A22B-Thinking-2507".into(),
            api: "openai-completions".into(),
            provider: "huggingface".into(),
            base_url: "https://router.huggingface.co/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.3,
                output: 3.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 262144,
            max_tokens: 131072,
            headers: None,
            compat: Some(json!({"supportsDeveloperRole":false})),
        },
        Model {
            id: "Qwen/Qwen3-Coder-480B-A35B-Instruct".into(),
            name: "Qwen3-Coder-480B-A35B-Instruct".into(),
            api: "openai-completions".into(),
            provider: "huggingface".into(),
            base_url: "https://router.huggingface.co/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 2.0,
                output: 2.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 262144,
            max_tokens: 66536,
            headers: None,
            compat: Some(json!({"supportsDeveloperRole":false})),
        },
        Model {
            id: "Qwen/Qwen3-Next-80B-A3B-Instruct".into(),
            name: "Qwen3-Next-80B-A3B-Instruct".into(),
            api: "openai-completions".into(),
            provider: "huggingface".into(),
            base_url: "https://router.huggingface.co/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.25,
                output: 1.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 262144,
            max_tokens: 66536,
            headers: None,
            compat: Some(json!({"supportsDeveloperRole":false})),
        },
        Model {
            id: "Qwen/Qwen3-Next-80B-A3B-Thinking".into(),
            name: "Qwen3-Next-80B-A3B-Thinking".into(),
            api: "openai-completions".into(),
            provider: "huggingface".into(),
            base_url: "https://router.huggingface.co/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.3,
                output: 2.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 262144,
            max_tokens: 131072,
            headers: None,
            compat: Some(json!({"supportsDeveloperRole":false})),
        },
        Model {
            id: "XiaomiMiMo/MiMo-V2-Flash".into(),
            name: "MiMo-V2-Flash".into(),
            api: "openai-completions".into(),
            provider: "huggingface".into(),
            base_url: "https://router.huggingface.co/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.1,
                output: 0.3,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 262144,
            max_tokens: 4096,
            headers: None,
            compat: Some(json!({"supportsDeveloperRole":false})),
        },
        Model {
            id: "deepseek-ai/DeepSeek-R1-0528".into(),
            name: "DeepSeek-R1-0528".into(),
            api: "openai-completions".into(),
            provider: "huggingface".into(),
            base_url: "https://router.huggingface.co/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 3.0,
                output: 5.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 163840,
            max_tokens: 163840,
            headers: None,
            compat: Some(json!({"supportsDeveloperRole":false})),
        },
        Model {
            id: "deepseek-ai/DeepSeek-V3.2".into(),
            name: "DeepSeek-V3.2".into(),
            api: "openai-completions".into(),
            provider: "huggingface".into(),
            base_url: "https://router.huggingface.co/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.28,
                output: 0.4,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 163840,
            max_tokens: 65536,
            headers: None,
            compat: Some(json!({"supportsDeveloperRole":false})),
        },
        Model {
            id: "moonshotai/Kimi-K2-Instruct".into(),
            name: "Kimi-K2-Instruct".into(),
            api: "openai-completions".into(),
            provider: "huggingface".into(),
            base_url: "https://router.huggingface.co/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 1.0,
                output: 3.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 16384,
            headers: None,
            compat: Some(json!({"supportsDeveloperRole":false})),
        },
        Model {
            id: "moonshotai/Kimi-K2-Instruct-0905".into(),
            name: "Kimi-K2-Instruct-0905".into(),
            api: "openai-completions".into(),
            provider: "huggingface".into(),
            base_url: "https://router.huggingface.co/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 1.0,
                output: 3.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 262144,
            max_tokens: 16384,
            headers: None,
            compat: Some(json!({"supportsDeveloperRole":false})),
        },
        Model {
            id: "moonshotai/Kimi-K2-Thinking".into(),
            name: "Kimi-K2-Thinking".into(),
            api: "openai-completions".into(),
            provider: "huggingface".into(),
            base_url: "https://router.huggingface.co/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.6,
                output: 2.5,
                cache_read: 0.15,
                cache_write: 0.0,
            },
            context_window: 262144,
            max_tokens: 262144,
            headers: None,
            compat: Some(json!({"supportsDeveloperRole":false})),
        },
        Model {
            id: "moonshotai/Kimi-K2.5".into(),
            name: "Kimi-K2.5".into(),
            api: "openai-completions".into(),
            provider: "huggingface".into(),
            base_url: "https://router.huggingface.co/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.6,
                output: 3.0,
                cache_read: 0.1,
                cache_write: 0.0,
            },
            context_window: 262144,
            max_tokens: 262144,
            headers: None,
            compat: Some(json!({"supportsDeveloperRole":false})),
        },
        Model {
            id: "zai-org/GLM-4.7".into(),
            name: "GLM-4.7".into(),
            api: "openai-completions".into(),
            provider: "huggingface".into(),
            base_url: "https://router.huggingface.co/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.6,
                output: 2.2,
                cache_read: 0.11,
                cache_write: 0.0,
            },
            context_window: 204800,
            max_tokens: 131072,
            headers: None,
            compat: Some(json!({"supportsDeveloperRole":false})),
        },
        Model {
            id: "zai-org/GLM-4.7-Flash".into(),
            name: "GLM-4.7-Flash".into(),
            api: "openai-completions".into(),
            provider: "huggingface".into(),
            base_url: "https://router.huggingface.co/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.0,
                output: 0.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 200000,
            max_tokens: 128000,
            headers: None,
            compat: Some(json!({"supportsDeveloperRole":false})),
        },
        Model {
            id: "k2p5".into(),
            name: "Kimi K2.5".into(),
            api: "anthropic-messages".into(),
            provider: "kimi-coding".into(),
            base_url: "https://api.kimi.com/coding".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.0,
                output: 0.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 262144,
            max_tokens: 32768,
            headers: None,
            compat: None,
        },
        Model {
            id: "kimi-k2-thinking".into(),
            name: "Kimi K2 Thinking".into(),
            api: "anthropic-messages".into(),
            provider: "kimi-coding".into(),
            base_url: "https://api.kimi.com/coding".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.0,
                output: 0.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 262144,
            max_tokens: 32768,
            headers: None,
            compat: None,
        },
        Model {
            id: "MiniMax-M2".into(),
            name: "MiniMax-M2".into(),
            api: "anthropic-messages".into(),
            provider: "minimax".into(),
            base_url: "https://api.minimax.io/anthropic".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.3,
                output: 1.2,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 196608,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "MiniMax-M2.1".into(),
            name: "MiniMax-M2.1".into(),
            api: "anthropic-messages".into(),
            provider: "minimax".into(),
            base_url: "https://api.minimax.io/anthropic".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.3,
                output: 1.2,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 204800,
            max_tokens: 131072,
            headers: None,
            compat: None,
        },
        Model {
            id: "MiniMax-M2".into(),
            name: "MiniMax-M2".into(),
            api: "anthropic-messages".into(),
            provider: "minimax-cn".into(),
            base_url: "https://api.minimaxi.com/anthropic".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.3,
                output: 1.2,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 196608,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "MiniMax-M2.1".into(),
            name: "MiniMax-M2.1".into(),
            api: "anthropic-messages".into(),
            provider: "minimax-cn".into(),
            base_url: "https://api.minimaxi.com/anthropic".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.3,
                output: 1.2,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 204800,
            max_tokens: 131072,
            headers: None,
            compat: None,
        },
        Model {
            id: "codestral-latest".into(),
            name: "Codestral".into(),
            api: "openai-completions".into(),
            provider: "mistral".into(),
            base_url: "https://api.mistral.ai/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.3,
                output: 0.9,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 256000,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "devstral-2512".into(),
            name: "Devstral 2".into(),
            api: "openai-completions".into(),
            provider: "mistral".into(),
            base_url: "https://api.mistral.ai/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.4,
                output: 2.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 262144,
            max_tokens: 262144,
            headers: None,
            compat: None,
        },
        Model {
            id: "devstral-medium-2507".into(),
            name: "Devstral Medium".into(),
            api: "openai-completions".into(),
            provider: "mistral".into(),
            base_url: "https://api.mistral.ai/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.4,
                output: 2.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "devstral-medium-latest".into(),
            name: "Devstral 2".into(),
            api: "openai-completions".into(),
            provider: "mistral".into(),
            base_url: "https://api.mistral.ai/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.4,
                output: 2.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 262144,
            max_tokens: 262144,
            headers: None,
            compat: None,
        },
        Model {
            id: "devstral-small-2505".into(),
            name: "Devstral Small 2505".into(),
            api: "openai-completions".into(),
            provider: "mistral".into(),
            base_url: "https://api.mistral.ai/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.1,
                output: 0.3,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "devstral-small-2507".into(),
            name: "Devstral Small".into(),
            api: "openai-completions".into(),
            provider: "mistral".into(),
            base_url: "https://api.mistral.ai/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.1,
                output: 0.3,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "labs-devstral-small-2512".into(),
            name: "Devstral Small 2".into(),
            api: "openai-completions".into(),
            provider: "mistral".into(),
            base_url: "https://api.mistral.ai/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.0,
                output: 0.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 256000,
            max_tokens: 256000,
            headers: None,
            compat: None,
        },
        Model {
            id: "magistral-medium-latest".into(),
            name: "Magistral Medium".into(),
            api: "openai-completions".into(),
            provider: "mistral".into(),
            base_url: "https://api.mistral.ai/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 2.0,
                output: 5.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 16384,
            headers: None,
            compat: None,
        },
        Model {
            id: "magistral-small".into(),
            name: "Magistral Small".into(),
            api: "openai-completions".into(),
            provider: "mistral".into(),
            base_url: "https://api.mistral.ai/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.5,
                output: 1.5,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "ministral-3b-latest".into(),
            name: "Ministral 3B".into(),
            api: "openai-completions".into(),
            provider: "mistral".into(),
            base_url: "https://api.mistral.ai/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.04,
                output: 0.04,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "ministral-8b-latest".into(),
            name: "Ministral 8B".into(),
            api: "openai-completions".into(),
            provider: "mistral".into(),
            base_url: "https://api.mistral.ai/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.1,
                output: 0.1,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "mistral-large-2411".into(),
            name: "Mistral Large 2.1".into(),
            api: "openai-completions".into(),
            provider: "mistral".into(),
            base_url: "https://api.mistral.ai/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 2.0,
                output: 6.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 16384,
            headers: None,
            compat: None,
        },
        Model {
            id: "mistral-large-2512".into(),
            name: "Mistral Large 3".into(),
            api: "openai-completions".into(),
            provider: "mistral".into(),
            base_url: "https://api.mistral.ai/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.5,
                output: 1.5,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 262144,
            max_tokens: 262144,
            headers: None,
            compat: None,
        },
        Model {
            id: "mistral-large-latest".into(),
            name: "Mistral Large".into(),
            api: "openai-completions".into(),
            provider: "mistral".into(),
            base_url: "https://api.mistral.ai/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.5,
                output: 1.5,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 262144,
            max_tokens: 262144,
            headers: None,
            compat: None,
        },
        Model {
            id: "mistral-medium-2505".into(),
            name: "Mistral Medium 3".into(),
            api: "openai-completions".into(),
            provider: "mistral".into(),
            base_url: "https://api.mistral.ai/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.4,
                output: 2.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 131072,
            headers: None,
            compat: None,
        },
        Model {
            id: "mistral-medium-2508".into(),
            name: "Mistral Medium 3.1".into(),
            api: "openai-completions".into(),
            provider: "mistral".into(),
            base_url: "https://api.mistral.ai/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.4,
                output: 2.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 262144,
            max_tokens: 262144,
            headers: None,
            compat: None,
        },
        Model {
            id: "mistral-medium-latest".into(),
            name: "Mistral Medium".into(),
            api: "openai-completions".into(),
            provider: "mistral".into(),
            base_url: "https://api.mistral.ai/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.4,
                output: 2.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 16384,
            headers: None,
            compat: None,
        },
        Model {
            id: "mistral-nemo".into(),
            name: "Mistral Nemo".into(),
            api: "openai-completions".into(),
            provider: "mistral".into(),
            base_url: "https://api.mistral.ai/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.15,
                output: 0.15,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "mistral-small-2506".into(),
            name: "Mistral Small 3.2".into(),
            api: "openai-completions".into(),
            provider: "mistral".into(),
            base_url: "https://api.mistral.ai/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.1,
                output: 0.3,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 16384,
            headers: None,
            compat: None,
        },
        Model {
            id: "mistral-small-latest".into(),
            name: "Mistral Small".into(),
            api: "openai-completions".into(),
            provider: "mistral".into(),
            base_url: "https://api.mistral.ai/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.1,
                output: 0.3,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 16384,
            headers: None,
            compat: None,
        },
        Model {
            id: "open-mistral-7b".into(),
            name: "Mistral 7B".into(),
            api: "openai-completions".into(),
            provider: "mistral".into(),
            base_url: "https://api.mistral.ai/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.25,
                output: 0.25,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 8000,
            max_tokens: 8000,
            headers: None,
            compat: None,
        },
        Model {
            id: "open-mixtral-8x22b".into(),
            name: "Mixtral 8x22B".into(),
            api: "openai-completions".into(),
            provider: "mistral".into(),
            base_url: "https://api.mistral.ai/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 2.0,
                output: 6.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 64000,
            max_tokens: 64000,
            headers: None,
            compat: None,
        },
        Model {
            id: "open-mixtral-8x7b".into(),
            name: "Mixtral 8x7B".into(),
            api: "openai-completions".into(),
            provider: "mistral".into(),
            base_url: "https://api.mistral.ai/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.7,
                output: 0.7,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 32000,
            max_tokens: 32000,
            headers: None,
            compat: None,
        },
        Model {
            id: "pixtral-12b".into(),
            name: "Pixtral 12B".into(),
            api: "openai-completions".into(),
            provider: "mistral".into(),
            base_url: "https://api.mistral.ai/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.15,
                output: 0.15,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "pixtral-large-latest".into(),
            name: "Pixtral Large".into(),
            api: "openai-completions".into(),
            provider: "mistral".into(),
            base_url: "https://api.mistral.ai/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 2.0,
                output: 6.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "codex-mini-latest".into(),
            name: "Codex Mini".into(),
            api: "openai-responses".into(),
            provider: "openai".into(),
            base_url: "https://api.openai.com/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 1.5,
                output: 6.0,
                cache_read: 0.375,
                cache_write: 0.0,
            },
            context_window: 200000,
            max_tokens: 100000,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-4".into(),
            name: "GPT-4".into(),
            api: "openai-responses".into(),
            provider: "openai".into(),
            base_url: "https://api.openai.com/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 30.0,
                output: 60.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 8192,
            max_tokens: 8192,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-4-turbo".into(),
            name: "GPT-4 Turbo".into(),
            api: "openai-responses".into(),
            provider: "openai".into(),
            base_url: "https://api.openai.com/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 10.0,
                output: 30.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-4.1".into(),
            name: "GPT-4.1".into(),
            api: "openai-responses".into(),
            provider: "openai".into(),
            base_url: "https://api.openai.com/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 2.0,
                output: 8.0,
                cache_read: 0.5,
                cache_write: 0.0,
            },
            context_window: 1047576,
            max_tokens: 32768,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-4.1-mini".into(),
            name: "GPT-4.1 mini".into(),
            api: "openai-responses".into(),
            provider: "openai".into(),
            base_url: "https://api.openai.com/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.4,
                output: 1.6,
                cache_read: 0.1,
                cache_write: 0.0,
            },
            context_window: 1047576,
            max_tokens: 32768,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-4.1-nano".into(),
            name: "GPT-4.1 nano".into(),
            api: "openai-responses".into(),
            provider: "openai".into(),
            base_url: "https://api.openai.com/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.1,
                output: 0.4,
                cache_read: 0.03,
                cache_write: 0.0,
            },
            context_window: 1047576,
            max_tokens: 32768,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-4o".into(),
            name: "GPT-4o".into(),
            api: "openai-responses".into(),
            provider: "openai".into(),
            base_url: "https://api.openai.com/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 2.5,
                output: 10.0,
                cache_read: 1.25,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 16384,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-4o-2024-05-13".into(),
            name: "GPT-4o (2024-05-13)".into(),
            api: "openai-responses".into(),
            provider: "openai".into(),
            base_url: "https://api.openai.com/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 5.0,
                output: 15.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-4o-2024-08-06".into(),
            name: "GPT-4o (2024-08-06)".into(),
            api: "openai-responses".into(),
            provider: "openai".into(),
            base_url: "https://api.openai.com/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 2.5,
                output: 10.0,
                cache_read: 1.25,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 16384,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-4o-2024-11-20".into(),
            name: "GPT-4o (2024-11-20)".into(),
            api: "openai-responses".into(),
            provider: "openai".into(),
            base_url: "https://api.openai.com/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 2.5,
                output: 10.0,
                cache_read: 1.25,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 16384,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-4o-mini".into(),
            name: "GPT-4o mini".into(),
            api: "openai-responses".into(),
            provider: "openai".into(),
            base_url: "https://api.openai.com/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.15,
                output: 0.6,
                cache_read: 0.08,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 16384,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-5".into(),
            name: "GPT-5".into(),
            api: "openai-responses".into(),
            provider: "openai".into(),
            base_url: "https://api.openai.com/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.25,
                output: 10.0,
                cache_read: 0.125,
                cache_write: 0.0,
            },
            context_window: 400000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-5-chat-latest".into(),
            name: "GPT-5 Chat Latest".into(),
            api: "openai-responses".into(),
            provider: "openai".into(),
            base_url: "https://api.openai.com/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.25,
                output: 10.0,
                cache_read: 0.125,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 16384,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-5-codex".into(),
            name: "GPT-5-Codex".into(),
            api: "openai-responses".into(),
            provider: "openai".into(),
            base_url: "https://api.openai.com/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.25,
                output: 10.0,
                cache_read: 0.125,
                cache_write: 0.0,
            },
            context_window: 400000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-5-mini".into(),
            name: "GPT-5 Mini".into(),
            api: "openai-responses".into(),
            provider: "openai".into(),
            base_url: "https://api.openai.com/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.25,
                output: 2.0,
                cache_read: 0.025,
                cache_write: 0.0,
            },
            context_window: 400000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-5-nano".into(),
            name: "GPT-5 Nano".into(),
            api: "openai-responses".into(),
            provider: "openai".into(),
            base_url: "https://api.openai.com/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.05,
                output: 0.4,
                cache_read: 0.005,
                cache_write: 0.0,
            },
            context_window: 400000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-5-pro".into(),
            name: "GPT-5 Pro".into(),
            api: "openai-responses".into(),
            provider: "openai".into(),
            base_url: "https://api.openai.com/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 15.0,
                output: 120.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 400000,
            max_tokens: 272000,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-5.1".into(),
            name: "GPT-5.1".into(),
            api: "openai-responses".into(),
            provider: "openai".into(),
            base_url: "https://api.openai.com/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.25,
                output: 10.0,
                cache_read: 0.13,
                cache_write: 0.0,
            },
            context_window: 400000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-5.1-chat-latest".into(),
            name: "GPT-5.1 Chat".into(),
            api: "openai-responses".into(),
            provider: "openai".into(),
            base_url: "https://api.openai.com/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.25,
                output: 10.0,
                cache_read: 0.125,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 16384,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-5.1-codex".into(),
            name: "GPT-5.1 Codex".into(),
            api: "openai-responses".into(),
            provider: "openai".into(),
            base_url: "https://api.openai.com/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.25,
                output: 10.0,
                cache_read: 0.125,
                cache_write: 0.0,
            },
            context_window: 400000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-5.1-codex-max".into(),
            name: "GPT-5.1 Codex Max".into(),
            api: "openai-responses".into(),
            provider: "openai".into(),
            base_url: "https://api.openai.com/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.25,
                output: 10.0,
                cache_read: 0.125,
                cache_write: 0.0,
            },
            context_window: 400000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-5.1-codex-mini".into(),
            name: "GPT-5.1 Codex mini".into(),
            api: "openai-responses".into(),
            provider: "openai".into(),
            base_url: "https://api.openai.com/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.25,
                output: 2.0,
                cache_read: 0.025,
                cache_write: 0.0,
            },
            context_window: 400000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-5.2".into(),
            name: "GPT-5.2".into(),
            api: "openai-responses".into(),
            provider: "openai".into(),
            base_url: "https://api.openai.com/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.75,
                output: 14.0,
                cache_read: 0.175,
                cache_write: 0.0,
            },
            context_window: 400000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-5.2-chat-latest".into(),
            name: "GPT-5.2 Chat".into(),
            api: "openai-responses".into(),
            provider: "openai".into(),
            base_url: "https://api.openai.com/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.75,
                output: 14.0,
                cache_read: 0.175,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 16384,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-5.2-codex".into(),
            name: "GPT-5.2 Codex".into(),
            api: "openai-responses".into(),
            provider: "openai".into(),
            base_url: "https://api.openai.com/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.75,
                output: 14.0,
                cache_read: 0.175,
                cache_write: 0.0,
            },
            context_window: 400000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-5.2-pro".into(),
            name: "GPT-5.2 Pro".into(),
            api: "openai-responses".into(),
            provider: "openai".into(),
            base_url: "https://api.openai.com/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 21.0,
                output: 168.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 400000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-5.3-codex".into(),
            name: "GPT-5.3 Codex".into(),
            api: "openai-responses".into(),
            provider: "openai".into(),
            base_url: "https://api.openai.com/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.75,
                output: 14.0,
                cache_read: 0.175,
                cache_write: 0.0,
            },
            context_window: 400000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "o1".into(),
            name: "o1".into(),
            api: "openai-responses".into(),
            provider: "openai".into(),
            base_url: "https://api.openai.com/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 15.0,
                output: 60.0,
                cache_read: 7.5,
                cache_write: 0.0,
            },
            context_window: 200000,
            max_tokens: 100000,
            headers: None,
            compat: None,
        },
        Model {
            id: "o1-pro".into(),
            name: "o1-pro".into(),
            api: "openai-responses".into(),
            provider: "openai".into(),
            base_url: "https://api.openai.com/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 150.0,
                output: 600.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 200000,
            max_tokens: 100000,
            headers: None,
            compat: None,
        },
        Model {
            id: "o3".into(),
            name: "o3".into(),
            api: "openai-responses".into(),
            provider: "openai".into(),
            base_url: "https://api.openai.com/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 2.0,
                output: 8.0,
                cache_read: 0.5,
                cache_write: 0.0,
            },
            context_window: 200000,
            max_tokens: 100000,
            headers: None,
            compat: None,
        },
        Model {
            id: "o3-deep-research".into(),
            name: "o3-deep-research".into(),
            api: "openai-responses".into(),
            provider: "openai".into(),
            base_url: "https://api.openai.com/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 10.0,
                output: 40.0,
                cache_read: 2.5,
                cache_write: 0.0,
            },
            context_window: 200000,
            max_tokens: 100000,
            headers: None,
            compat: None,
        },
        Model {
            id: "o3-mini".into(),
            name: "o3-mini".into(),
            api: "openai-responses".into(),
            provider: "openai".into(),
            base_url: "https://api.openai.com/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 1.1,
                output: 4.4,
                cache_read: 0.55,
                cache_write: 0.0,
            },
            context_window: 200000,
            max_tokens: 100000,
            headers: None,
            compat: None,
        },
        Model {
            id: "o3-pro".into(),
            name: "o3-pro".into(),
            api: "openai-responses".into(),
            provider: "openai".into(),
            base_url: "https://api.openai.com/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 20.0,
                output: 80.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 200000,
            max_tokens: 100000,
            headers: None,
            compat: None,
        },
        Model {
            id: "o4-mini".into(),
            name: "o4-mini".into(),
            api: "openai-responses".into(),
            provider: "openai".into(),
            base_url: "https://api.openai.com/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.1,
                output: 4.4,
                cache_read: 0.28,
                cache_write: 0.0,
            },
            context_window: 200000,
            max_tokens: 100000,
            headers: None,
            compat: None,
        },
        Model {
            id: "o4-mini-deep-research".into(),
            name: "o4-mini-deep-research".into(),
            api: "openai-responses".into(),
            provider: "openai".into(),
            base_url: "https://api.openai.com/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 2.0,
                output: 8.0,
                cache_read: 0.5,
                cache_write: 0.0,
            },
            context_window: 200000,
            max_tokens: 100000,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-5.1".into(),
            name: "GPT-5.1".into(),
            api: "openai-codex-responses".into(),
            provider: "openai-codex".into(),
            base_url: "https://chatgpt.com/backend-api".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.25,
                output: 10.0,
                cache_read: 0.125,
                cache_write: 0.0,
            },
            context_window: 272000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-5.1-codex-max".into(),
            name: "GPT-5.1 Codex Max".into(),
            api: "openai-codex-responses".into(),
            provider: "openai-codex".into(),
            base_url: "https://chatgpt.com/backend-api".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.25,
                output: 10.0,
                cache_read: 0.125,
                cache_write: 0.0,
            },
            context_window: 272000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-5.1-codex-mini".into(),
            name: "GPT-5.1 Codex Mini".into(),
            api: "openai-codex-responses".into(),
            provider: "openai-codex".into(),
            base_url: "https://chatgpt.com/backend-api".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.25,
                output: 2.0,
                cache_read: 0.025,
                cache_write: 0.0,
            },
            context_window: 272000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-5.2".into(),
            name: "GPT-5.2".into(),
            api: "openai-codex-responses".into(),
            provider: "openai-codex".into(),
            base_url: "https://chatgpt.com/backend-api".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.75,
                output: 14.0,
                cache_read: 0.175,
                cache_write: 0.0,
            },
            context_window: 272000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-5.2-codex".into(),
            name: "GPT-5.2 Codex".into(),
            api: "openai-codex-responses".into(),
            provider: "openai-codex".into(),
            base_url: "https://chatgpt.com/backend-api".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.75,
                output: 14.0,
                cache_read: 0.175,
                cache_write: 0.0,
            },
            context_window: 272000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-5.3-codex".into(),
            name: "GPT-5.3 Codex".into(),
            api: "openai-codex-responses".into(),
            provider: "openai-codex".into(),
            base_url: "https://chatgpt.com/backend-api".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.75,
                output: 14.0,
                cache_read: 0.175,
                cache_write: 0.0,
            },
            context_window: 272000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "big-pickle".into(),
            name: "Big Pickle".into(),
            api: "openai-completions".into(),
            provider: "opencode".into(),
            base_url: "https://opencode.ai/zen/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.0,
                output: 0.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 200000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "claude-3-5-haiku".into(),
            name: "Claude Haiku 3.5".into(),
            api: "anthropic-messages".into(),
            provider: "opencode".into(),
            base_url: "https://opencode.ai/zen".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.8,
                output: 4.0,
                cache_read: 0.08,
                cache_write: 1.0,
            },
            context_window: 200000,
            max_tokens: 8192,
            headers: None,
            compat: None,
        },
        Model {
            id: "claude-haiku-4-5".into(),
            name: "Claude Haiku 4.5".into(),
            api: "anthropic-messages".into(),
            provider: "opencode".into(),
            base_url: "https://opencode.ai/zen".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.0,
                output: 5.0,
                cache_read: 0.1,
                cache_write: 1.25,
            },
            context_window: 200000,
            max_tokens: 64000,
            headers: None,
            compat: None,
        },
        Model {
            id: "claude-opus-4-1".into(),
            name: "Claude Opus 4.1".into(),
            api: "anthropic-messages".into(),
            provider: "opencode".into(),
            base_url: "https://opencode.ai/zen".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 15.0,
                output: 75.0,
                cache_read: 1.5,
                cache_write: 18.75,
            },
            context_window: 200000,
            max_tokens: 32000,
            headers: None,
            compat: None,
        },
        Model {
            id: "claude-opus-4-5".into(),
            name: "Claude Opus 4.5".into(),
            api: "anthropic-messages".into(),
            provider: "opencode".into(),
            base_url: "https://opencode.ai/zen".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 5.0,
                output: 25.0,
                cache_read: 0.5,
                cache_write: 6.25,
            },
            context_window: 200000,
            max_tokens: 64000,
            headers: None,
            compat: None,
        },
        Model {
            id: "claude-opus-4-6".into(),
            name: "Claude Opus 4.6".into(),
            api: "anthropic-messages".into(),
            provider: "opencode".into(),
            base_url: "https://opencode.ai/zen".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 5.0,
                output: 25.0,
                cache_read: 0.5,
                cache_write: 6.25,
            },
            context_window: 200000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "claude-sonnet-4".into(),
            name: "Claude Sonnet 4".into(),
            api: "anthropic-messages".into(),
            provider: "opencode".into(),
            base_url: "https://opencode.ai/zen".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 3.0,
                output: 15.0,
                cache_read: 0.3,
                cache_write: 3.75,
            },
            context_window: 200000,
            max_tokens: 64000,
            headers: None,
            compat: None,
        },
        Model {
            id: "claude-sonnet-4-5".into(),
            name: "Claude Sonnet 4.5".into(),
            api: "anthropic-messages".into(),
            provider: "opencode".into(),
            base_url: "https://opencode.ai/zen".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 3.0,
                output: 15.0,
                cache_read: 0.3,
                cache_write: 3.75,
            },
            context_window: 200000,
            max_tokens: 64000,
            headers: None,
            compat: None,
        },
        Model {
            id: "gemini-3-flash".into(),
            name: "Gemini 3 Flash".into(),
            api: "google-generative-ai".into(),
            provider: "opencode".into(),
            base_url: "https://opencode.ai/zen/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.5,
                output: 3.0,
                cache_read: 0.05,
                cache_write: 0.0,
            },
            context_window: 1048576,
            max_tokens: 65536,
            headers: None,
            compat: None,
        },
        Model {
            id: "gemini-3-pro".into(),
            name: "Gemini 3 Pro".into(),
            api: "google-generative-ai".into(),
            provider: "opencode".into(),
            base_url: "https://opencode.ai/zen/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 2.0,
                output: 12.0,
                cache_read: 0.2,
                cache_write: 0.0,
            },
            context_window: 1048576,
            max_tokens: 65536,
            headers: None,
            compat: None,
        },
        Model {
            id: "glm-4.6".into(),
            name: "GLM-4.6".into(),
            api: "openai-completions".into(),
            provider: "opencode".into(),
            base_url: "https://opencode.ai/zen/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.6,
                output: 2.2,
                cache_read: 0.1,
                cache_write: 0.0,
            },
            context_window: 204800,
            max_tokens: 131072,
            headers: None,
            compat: None,
        },
        Model {
            id: "glm-4.7".into(),
            name: "GLM-4.7".into(),
            api: "openai-completions".into(),
            provider: "opencode".into(),
            base_url: "https://opencode.ai/zen/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.6,
                output: 2.2,
                cache_read: 0.1,
                cache_write: 0.0,
            },
            context_window: 204800,
            max_tokens: 131072,
            headers: None,
            compat: None,
        },
        Model {
            id: "glm-4.7-free".into(),
            name: "GLM-4.7 Free".into(),
            api: "openai-completions".into(),
            provider: "opencode".into(),
            base_url: "https://opencode.ai/zen/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.0,
                output: 0.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 204800,
            max_tokens: 131072,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-5".into(),
            name: "GPT-5".into(),
            api: "openai-responses".into(),
            provider: "opencode".into(),
            base_url: "https://opencode.ai/zen/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.07,
                output: 8.5,
                cache_read: 0.107,
                cache_write: 0.0,
            },
            context_window: 400000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-5-codex".into(),
            name: "GPT-5 Codex".into(),
            api: "openai-responses".into(),
            provider: "opencode".into(),
            base_url: "https://opencode.ai/zen/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.07,
                output: 8.5,
                cache_read: 0.107,
                cache_write: 0.0,
            },
            context_window: 400000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-5-nano".into(),
            name: "GPT-5 Nano".into(),
            api: "openai-responses".into(),
            provider: "opencode".into(),
            base_url: "https://opencode.ai/zen/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.0,
                output: 0.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 400000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-5.1".into(),
            name: "GPT-5.1".into(),
            api: "openai-responses".into(),
            provider: "opencode".into(),
            base_url: "https://opencode.ai/zen/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.07,
                output: 8.5,
                cache_read: 0.107,
                cache_write: 0.0,
            },
            context_window: 400000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-5.1-codex".into(),
            name: "GPT-5.1 Codex".into(),
            api: "openai-responses".into(),
            provider: "opencode".into(),
            base_url: "https://opencode.ai/zen/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.07,
                output: 8.5,
                cache_read: 0.107,
                cache_write: 0.0,
            },
            context_window: 400000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-5.1-codex-max".into(),
            name: "GPT-5.1 Codex Max".into(),
            api: "openai-responses".into(),
            provider: "opencode".into(),
            base_url: "https://opencode.ai/zen/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.25,
                output: 10.0,
                cache_read: 0.125,
                cache_write: 0.0,
            },
            context_window: 400000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-5.1-codex-mini".into(),
            name: "GPT-5.1 Codex Mini".into(),
            api: "openai-responses".into(),
            provider: "opencode".into(),
            base_url: "https://opencode.ai/zen/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.25,
                output: 2.0,
                cache_read: 0.025,
                cache_write: 0.0,
            },
            context_window: 400000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-5.2".into(),
            name: "GPT-5.2".into(),
            api: "openai-responses".into(),
            provider: "opencode".into(),
            base_url: "https://opencode.ai/zen/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.75,
                output: 14.0,
                cache_read: 0.175,
                cache_write: 0.0,
            },
            context_window: 400000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "gpt-5.2-codex".into(),
            name: "GPT-5.2 Codex".into(),
            api: "openai-responses".into(),
            provider: "opencode".into(),
            base_url: "https://opencode.ai/zen/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.75,
                output: 14.0,
                cache_read: 0.175,
                cache_write: 0.0,
            },
            context_window: 400000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "kimi-k2".into(),
            name: "Kimi K2".into(),
            api: "openai-completions".into(),
            provider: "opencode".into(),
            base_url: "https://opencode.ai/zen/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.4,
                output: 2.5,
                cache_read: 0.4,
                cache_write: 0.0,
            },
            context_window: 262144,
            max_tokens: 262144,
            headers: None,
            compat: None,
        },
        Model {
            id: "kimi-k2-thinking".into(),
            name: "Kimi K2 Thinking".into(),
            api: "openai-completions".into(),
            provider: "opencode".into(),
            base_url: "https://opencode.ai/zen/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.4,
                output: 2.5,
                cache_read: 0.4,
                cache_write: 0.0,
            },
            context_window: 262144,
            max_tokens: 262144,
            headers: None,
            compat: None,
        },
        Model {
            id: "kimi-k2.5".into(),
            name: "Kimi K2.5".into(),
            api: "openai-completions".into(),
            provider: "opencode".into(),
            base_url: "https://opencode.ai/zen/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.6,
                output: 3.0,
                cache_read: 0.08,
                cache_write: 0.0,
            },
            context_window: 262144,
            max_tokens: 262144,
            headers: None,
            compat: None,
        },
        Model {
            id: "kimi-k2.5-free".into(),
            name: "Kimi K2.5 Free".into(),
            api: "openai-completions".into(),
            provider: "opencode".into(),
            base_url: "https://opencode.ai/zen/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.0,
                output: 0.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 262144,
            max_tokens: 262144,
            headers: None,
            compat: None,
        },
        Model {
            id: "minimax-m2.1".into(),
            name: "MiniMax M2.1".into(),
            api: "openai-completions".into(),
            provider: "opencode".into(),
            base_url: "https://opencode.ai/zen/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.3,
                output: 1.2,
                cache_read: 0.1,
                cache_write: 0.0,
            },
            context_window: 204800,
            max_tokens: 131072,
            headers: None,
            compat: None,
        },
        Model {
            id: "minimax-m2.1-free".into(),
            name: "MiniMax M2.1 Free".into(),
            api: "anthropic-messages".into(),
            provider: "opencode".into(),
            base_url: "https://opencode.ai/zen".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.0,
                output: 0.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 204800,
            max_tokens: 131072,
            headers: None,
            compat: None,
        },
        Model {
            id: "qwen3-coder".into(),
            name: "Qwen3 Coder".into(),
            api: "openai-completions".into(),
            provider: "opencode".into(),
            base_url: "https://opencode.ai/zen/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.45,
                output: 1.8,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 262144,
            max_tokens: 65536,
            headers: None,
            compat: None,
        },
        Model {
            id: "trinity-large-preview-free".into(),
            name: "Trinity Large Preview".into(),
            api: "openai-completions".into(),
            provider: "opencode".into(),
            base_url: "https://opencode.ai/zen/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.0,
                output: 0.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 131072,
            headers: None,
            compat: None,
        },
        Model {
            id: "ai21/jamba-large-1.7".into(),
            name: "AI21: Jamba Large 1.7".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 2.0,
                output: 8.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 256000,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "ai21/jamba-mini-1.7".into(),
            name: "AI21: Jamba Mini 1.7".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.19999999999999998,
                output: 0.39999999999999997,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 256000,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "alibaba/tongyi-deepresearch-30b-a3b".into(),
            name: "Tongyi DeepResearch 30B A3B".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.09,
                output: 0.44999999999999996,
                cache_read: 0.09,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 131072,
            headers: None,
            compat: None,
        },
        Model {
            id: "allenai/olmo-3.1-32b-instruct".into(),
            name: "AllenAI: Olmo 3.1 32B Instruct".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.19999999999999998,
                output: 0.6,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 65536,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "amazon/nova-2-lite-v1".into(),
            name: "Amazon: Nova 2 Lite".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.3,
                output: 2.5,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 1000000,
            max_tokens: 65535,
            headers: None,
            compat: None,
        },
        Model {
            id: "amazon/nova-lite-v1".into(),
            name: "Amazon: Nova Lite 1.0".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.06,
                output: 0.24,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 300000,
            max_tokens: 5120,
            headers: None,
            compat: None,
        },
        Model {
            id: "amazon/nova-micro-v1".into(),
            name: "Amazon: Nova Micro 1.0".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.035,
                output: 0.14,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 5120,
            headers: None,
            compat: None,
        },
        Model {
            id: "amazon/nova-premier-v1".into(),
            name: "Amazon: Nova Premier 1.0".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 2.5,
                output: 12.5,
                cache_read: 0.625,
                cache_write: 0.0,
            },
            context_window: 1000000,
            max_tokens: 32000,
            headers: None,
            compat: None,
        },
        Model {
            id: "amazon/nova-pro-v1".into(),
            name: "Amazon: Nova Pro 1.0".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.7999999999999999,
                output: 3.1999999999999997,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 300000,
            max_tokens: 5120,
            headers: None,
            compat: None,
        },
        Model {
            id: "anthropic/claude-3-haiku".into(),
            name: "Anthropic: Claude 3 Haiku".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.25,
                output: 1.25,
                cache_read: 0.03,
                cache_write: 0.3,
            },
            context_window: 200000,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "anthropic/claude-3.5-haiku".into(),
            name: "Anthropic: Claude 3.5 Haiku".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.7999999999999999,
                output: 4.0,
                cache_read: 0.08,
                cache_write: 1.0,
            },
            context_window: 200000,
            max_tokens: 8192,
            headers: None,
            compat: None,
        },
        Model {
            id: "anthropic/claude-3.5-sonnet".into(),
            name: "Anthropic: Claude 3.5 Sonnet".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 6.0,
                output: 30.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 200000,
            max_tokens: 8192,
            headers: None,
            compat: None,
        },
        Model {
            id: "anthropic/claude-3.7-sonnet".into(),
            name: "Anthropic: Claude 3.7 Sonnet".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 3.0,
                output: 15.0,
                cache_read: 0.3,
                cache_write: 3.75,
            },
            context_window: 200000,
            max_tokens: 64000,
            headers: None,
            compat: None,
        },
        Model {
            id: "anthropic/claude-3.7-sonnet:thinking".into(),
            name: "Anthropic: Claude 3.7 Sonnet (thinking)".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 3.0,
                output: 15.0,
                cache_read: 0.3,
                cache_write: 3.75,
            },
            context_window: 200000,
            max_tokens: 64000,
            headers: None,
            compat: None,
        },
        Model {
            id: "anthropic/claude-haiku-4.5".into(),
            name: "Anthropic: Claude Haiku 4.5".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.0,
                output: 5.0,
                cache_read: 0.09999999999999999,
                cache_write: 1.25,
            },
            context_window: 200000,
            max_tokens: 64000,
            headers: None,
            compat: None,
        },
        Model {
            id: "anthropic/claude-opus-4".into(),
            name: "Anthropic: Claude Opus 4".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 15.0,
                output: 75.0,
                cache_read: 1.5,
                cache_write: 18.75,
            },
            context_window: 200000,
            max_tokens: 32000,
            headers: None,
            compat: None,
        },
        Model {
            id: "anthropic/claude-opus-4.1".into(),
            name: "Anthropic: Claude Opus 4.1".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 15.0,
                output: 75.0,
                cache_read: 1.5,
                cache_write: 18.75,
            },
            context_window: 200000,
            max_tokens: 32000,
            headers: None,
            compat: None,
        },
        Model {
            id: "anthropic/claude-opus-4.5".into(),
            name: "Anthropic: Claude Opus 4.5".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 5.0,
                output: 25.0,
                cache_read: 0.5,
                cache_write: 6.25,
            },
            context_window: 200000,
            max_tokens: 64000,
            headers: None,
            compat: None,
        },
        Model {
            id: "anthropic/claude-opus-4.6".into(),
            name: "Anthropic: Claude Opus 4.6".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 5.0,
                output: 25.0,
                cache_read: 0.5,
                cache_write: 6.25,
            },
            context_window: 1000000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "anthropic/claude-sonnet-4".into(),
            name: "Anthropic: Claude Sonnet 4".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 3.0,
                output: 15.0,
                cache_read: 0.3,
                cache_write: 3.75,
            },
            context_window: 1000000,
            max_tokens: 64000,
            headers: None,
            compat: None,
        },
        Model {
            id: "anthropic/claude-sonnet-4.5".into(),
            name: "Anthropic: Claude Sonnet 4.5".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 3.0,
                output: 15.0,
                cache_read: 0.3,
                cache_write: 3.75,
            },
            context_window: 1000000,
            max_tokens: 64000,
            headers: None,
            compat: None,
        },
        Model {
            id: "arcee-ai/trinity-large-preview:free".into(),
            name: "Arcee AI: Trinity Large Preview (free)".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.0,
                output: 0.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131000,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "arcee-ai/trinity-mini".into(),
            name: "Arcee AI: Trinity Mini".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.045,
                output: 0.15,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 131072,
            headers: None,
            compat: None,
        },
        Model {
            id: "arcee-ai/trinity-mini:free".into(),
            name: "Arcee AI: Trinity Mini (free)".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.0,
                output: 0.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "arcee-ai/virtuoso-large".into(),
            name: "Arcee AI: Virtuoso Large".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.75,
                output: 1.2,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 64000,
            headers: None,
            compat: None,
        },
        Model {
            id: "auto".into(),
            name: "Auto".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.0,
                output: 0.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 2000000,
            max_tokens: 30000,
            headers: None,
            compat: None,
        },
        Model {
            id: "baidu/ernie-4.5-21b-a3b".into(),
            name: "Baidu: ERNIE 4.5 21B A3B".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.07,
                output: 0.28,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 120000,
            max_tokens: 8000,
            headers: None,
            compat: None,
        },
        Model {
            id: "baidu/ernie-4.5-vl-28b-a3b".into(),
            name: "Baidu: ERNIE 4.5 VL 28B A3B".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.14,
                output: 0.56,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 30000,
            max_tokens: 8000,
            headers: None,
            compat: None,
        },
        Model {
            id: "bytedance-seed/seed-1.6".into(),
            name: "ByteDance Seed: Seed 1.6".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.25,
                output: 2.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 262144,
            max_tokens: 32768,
            headers: None,
            compat: None,
        },
        Model {
            id: "bytedance-seed/seed-1.6-flash".into(),
            name: "ByteDance Seed: Seed 1.6 Flash".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.075,
                output: 0.3,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 262144,
            max_tokens: 32768,
            headers: None,
            compat: None,
        },
        Model {
            id: "cohere/command-r-08-2024".into(),
            name: "Cohere: Command R (08-2024)".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.15,
                output: 0.6,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 4000,
            headers: None,
            compat: None,
        },
        Model {
            id: "cohere/command-r-plus-08-2024".into(),
            name: "Cohere: Command R+ (08-2024)".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 2.5,
                output: 10.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 4000,
            headers: None,
            compat: None,
        },
        Model {
            id: "deepseek/deepseek-chat".into(),
            name: "DeepSeek: DeepSeek V3".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.3,
                output: 1.2,
                cache_read: 0.15,
                cache_write: 0.0,
            },
            context_window: 163840,
            max_tokens: 163840,
            headers: None,
            compat: None,
        },
        Model {
            id: "deepseek/deepseek-chat-v3-0324".into(),
            name: "DeepSeek: DeepSeek V3 0324".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.19,
                output: 0.87,
                cache_read: 0.095,
                cache_write: 0.0,
            },
            context_window: 163840,
            max_tokens: 65536,
            headers: None,
            compat: None,
        },
        Model {
            id: "deepseek/deepseek-chat-v3.1".into(),
            name: "DeepSeek: DeepSeek V3.1".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.15,
                output: 0.75,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 32768,
            max_tokens: 7168,
            headers: None,
            compat: None,
        },
        Model {
            id: "deepseek/deepseek-r1".into(),
            name: "DeepSeek: R1".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.7,
                output: 2.5,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 64000,
            max_tokens: 16000,
            headers: None,
            compat: None,
        },
        Model {
            id: "deepseek/deepseek-r1-0528".into(),
            name: "DeepSeek: R1 0528".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.39999999999999997,
                output: 1.75,
                cache_read: 0.19999999999999998,
                cache_write: 0.0,
            },
            context_window: 163840,
            max_tokens: 65536,
            headers: None,
            compat: None,
        },
        Model {
            id: "deepseek/deepseek-v3.1-terminus".into(),
            name: "DeepSeek: DeepSeek V3.1 Terminus".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.21,
                output: 0.7899999999999999,
                cache_read: 0.1300000002,
                cache_write: 0.0,
            },
            context_window: 163840,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "deepseek/deepseek-v3.1-terminus:exacto".into(),
            name: "DeepSeek: DeepSeek V3.1 Terminus (exacto)".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.21,
                output: 0.7899999999999999,
                cache_read: 0.16799999999999998,
                cache_write: 0.0,
            },
            context_window: 163840,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "deepseek/deepseek-v3.2".into(),
            name: "DeepSeek: DeepSeek V3.2".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.25,
                output: 0.38,
                cache_read: 0.125,
                cache_write: 0.0,
            },
            context_window: 163840,
            max_tokens: 65536,
            headers: None,
            compat: None,
        },
        Model {
            id: "deepseek/deepseek-v3.2-exp".into(),
            name: "DeepSeek: DeepSeek V3.2 Exp".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.27,
                output: 0.41,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 163840,
            max_tokens: 65536,
            headers: None,
            compat: None,
        },
        Model {
            id: "google/gemini-2.0-flash-001".into(),
            name: "Google: Gemini 2.0 Flash".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.09999999999999999,
                output: 0.39999999999999997,
                cache_read: 0.024999999999999998,
                cache_write: 0.08333333333333334,
            },
            context_window: 1048576,
            max_tokens: 8192,
            headers: None,
            compat: None,
        },
        Model {
            id: "google/gemini-2.0-flash-lite-001".into(),
            name: "Google: Gemini 2.0 Flash Lite".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.075,
                output: 0.3,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 1048576,
            max_tokens: 8192,
            headers: None,
            compat: None,
        },
        Model {
            id: "google/gemini-2.5-flash".into(),
            name: "Google: Gemini 2.5 Flash".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.3,
                output: 2.5,
                cache_read: 0.03,
                cache_write: 0.08333333333333334,
            },
            context_window: 1048576,
            max_tokens: 65535,
            headers: None,
            compat: None,
        },
        Model {
            id: "google/gemini-2.5-flash-lite".into(),
            name: "Google: Gemini 2.5 Flash Lite".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.09999999999999999,
                output: 0.39999999999999997,
                cache_read: 0.01,
                cache_write: 0.08333333333333334,
            },
            context_window: 1048576,
            max_tokens: 65535,
            headers: None,
            compat: None,
        },
        Model {
            id: "google/gemini-2.5-flash-lite-preview-09-2025".into(),
            name: "Google: Gemini 2.5 Flash Lite Preview 09-2025".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.09999999999999999,
                output: 0.39999999999999997,
                cache_read: 0.01,
                cache_write: 0.08333333333333334,
            },
            context_window: 1048576,
            max_tokens: 65535,
            headers: None,
            compat: None,
        },
        Model {
            id: "google/gemini-2.5-flash-preview-09-2025".into(),
            name: "Google: Gemini 2.5 Flash Preview 09-2025".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.3,
                output: 2.5,
                cache_read: 0.03,
                cache_write: 0.08333333333333334,
            },
            context_window: 1048576,
            max_tokens: 65536,
            headers: None,
            compat: None,
        },
        Model {
            id: "google/gemini-2.5-pro".into(),
            name: "Google: Gemini 2.5 Pro".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.25,
                output: 10.0,
                cache_read: 0.125,
                cache_write: 0.375,
            },
            context_window: 1048576,
            max_tokens: 65536,
            headers: None,
            compat: None,
        },
        Model {
            id: "google/gemini-2.5-pro-preview".into(),
            name: "Google: Gemini 2.5 Pro Preview 06-05".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.25,
                output: 10.0,
                cache_read: 0.125,
                cache_write: 0.375,
            },
            context_window: 1048576,
            max_tokens: 65536,
            headers: None,
            compat: None,
        },
        Model {
            id: "google/gemini-2.5-pro-preview-05-06".into(),
            name: "Google: Gemini 2.5 Pro Preview 05-06".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.25,
                output: 10.0,
                cache_read: 0.125,
                cache_write: 0.375,
            },
            context_window: 1048576,
            max_tokens: 65535,
            headers: None,
            compat: None,
        },
        Model {
            id: "google/gemini-3-flash-preview".into(),
            name: "Google: Gemini 3 Flash Preview".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.5,
                output: 3.0,
                cache_read: 0.049999999999999996,
                cache_write: 0.08333333333333334,
            },
            context_window: 1048576,
            max_tokens: 65535,
            headers: None,
            compat: None,
        },
        Model {
            id: "google/gemini-3-pro-preview".into(),
            name: "Google: Gemini 3 Pro Preview".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 2.0,
                output: 12.0,
                cache_read: 0.19999999999999998,
                cache_write: 0.375,
            },
            context_window: 1048576,
            max_tokens: 65536,
            headers: None,
            compat: None,
        },
        Model {
            id: "google/gemma-3-27b-it".into(),
            name: "Google: Gemma 3 27B".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.04,
                output: 0.15,
                cache_read: 0.02,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 65536,
            headers: None,
            compat: None,
        },
        Model {
            id: "google/gemma-3-27b-it:free".into(),
            name: "Google: Gemma 3 27B (free)".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.0,
                output: 0.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 8192,
            headers: None,
            compat: None,
        },
        Model {
            id: "inception/mercury".into(),
            name: "Inception: Mercury".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.25,
                output: 1.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 16384,
            headers: None,
            compat: None,
        },
        Model {
            id: "inception/mercury-coder".into(),
            name: "Inception: Mercury Coder".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.25,
                output: 1.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 16384,
            headers: None,
            compat: None,
        },
        Model {
            id: "kwaipilot/kat-coder-pro".into(),
            name: "Kwaipilot: KAT-Coder-Pro V1".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.207,
                output: 0.828,
                cache_read: 0.0414,
                cache_write: 0.0,
            },
            context_window: 256000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "meta-llama/llama-3-8b-instruct".into(),
            name: "Meta: Llama 3 8B Instruct".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.03,
                output: 0.04,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 8192,
            max_tokens: 16384,
            headers: None,
            compat: None,
        },
        Model {
            id: "meta-llama/llama-3.1-405b-instruct".into(),
            name: "Meta: Llama 3.1 405B Instruct".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 4.0,
                output: 4.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131000,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "meta-llama/llama-3.1-70b-instruct".into(),
            name: "Meta: Llama 3.1 70B Instruct".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.39999999999999997,
                output: 0.39999999999999997,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "meta-llama/llama-3.1-8b-instruct".into(),
            name: "Meta: Llama 3.1 8B Instruct".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.02,
                output: 0.049999999999999996,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 16384,
            max_tokens: 16384,
            headers: None,
            compat: None,
        },
        Model {
            id: "meta-llama/llama-3.3-70b-instruct".into(),
            name: "Meta: Llama 3.3 70B Instruct".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.09999999999999999,
                output: 0.32,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 16384,
            headers: None,
            compat: None,
        },
        Model {
            id: "meta-llama/llama-3.3-70b-instruct:free".into(),
            name: "Meta: Llama 3.3 70B Instruct (free)".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.0,
                output: 0.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "meta-llama/llama-4-maverick".into(),
            name: "Meta: Llama 4 Maverick".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.15,
                output: 0.6,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 1048576,
            max_tokens: 16384,
            headers: None,
            compat: None,
        },
        Model {
            id: "meta-llama/llama-4-scout".into(),
            name: "Meta: Llama 4 Scout".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.08,
                output: 0.3,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 327680,
            max_tokens: 16384,
            headers: None,
            compat: None,
        },
        Model {
            id: "minimax/minimax-m1".into(),
            name: "MiniMax: MiniMax M1".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.39999999999999997,
                output: 2.2,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 1000000,
            max_tokens: 40000,
            headers: None,
            compat: None,
        },
        Model {
            id: "minimax/minimax-m2".into(),
            name: "MiniMax: MiniMax M2".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.255,
                output: 1.0,
                cache_read: 0.03,
                cache_write: 0.0,
            },
            context_window: 196608,
            max_tokens: 65536,
            headers: None,
            compat: None,
        },
        Model {
            id: "minimax/minimax-m2.1".into(),
            name: "MiniMax: MiniMax M2.1".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.27,
                output: 0.95,
                cache_read: 0.0299999997,
                cache_write: 0.0,
            },
            context_window: 196608,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "mistralai/codestral-2508".into(),
            name: "Mistral: Codestral 2508".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.3,
                output: 0.8999999999999999,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 256000,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "mistralai/devstral-2512".into(),
            name: "Mistral: Devstral 2 2512".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.049999999999999996,
                output: 0.22,
                cache_read: 0.024999999999999998,
                cache_write: 0.0,
            },
            context_window: 262144,
            max_tokens: 65536,
            headers: None,
            compat: None,
        },
        Model {
            id: "mistralai/devstral-medium".into(),
            name: "Mistral: Devstral Medium".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.39999999999999997,
                output: 2.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "mistralai/devstral-small".into(),
            name: "Mistral: Devstral Small 1.1".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.09999999999999999,
                output: 0.3,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "mistralai/ministral-14b-2512".into(),
            name: "Mistral: Ministral 3 14B 2512".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.19999999999999998,
                output: 0.19999999999999998,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 262144,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "mistralai/ministral-3b".into(),
            name: "Mistral: Ministral 3B".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.04,
                output: 0.04,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "mistralai/ministral-3b-2512".into(),
            name: "Mistral: Ministral 3 3B 2512".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.09999999999999999,
                output: 0.09999999999999999,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "mistralai/ministral-8b".into(),
            name: "Mistral: Ministral 8B".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.09999999999999999,
                output: 0.09999999999999999,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "mistralai/ministral-8b-2512".into(),
            name: "Mistral: Ministral 3 8B 2512".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.15,
                output: 0.15,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 262144,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "mistralai/mistral-large".into(),
            name: "Mistral Large".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 2.0,
                output: 6.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "mistralai/mistral-large-2407".into(),
            name: "Mistral Large 2407".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 2.0,
                output: 6.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "mistralai/mistral-large-2411".into(),
            name: "Mistral Large 2411".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 2.0,
                output: 6.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "mistralai/mistral-large-2512".into(),
            name: "Mistral: Mistral Large 3 2512".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.5,
                output: 1.5,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 262144,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "mistralai/mistral-medium-3".into(),
            name: "Mistral: Mistral Medium 3".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.39999999999999997,
                output: 2.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "mistralai/mistral-medium-3.1".into(),
            name: "Mistral: Mistral Medium 3.1".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.39999999999999997,
                output: 2.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "mistralai/mistral-nemo".into(),
            name: "Mistral: Mistral Nemo".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.02,
                output: 0.04,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 16384,
            headers: None,
            compat: None,
        },
        Model {
            id: "mistralai/mistral-saba".into(),
            name: "Mistral: Saba".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.19999999999999998,
                output: 0.6,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 32768,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "mistralai/mistral-small-24b-instruct-2501".into(),
            name: "Mistral: Mistral Small 3".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.049999999999999996,
                output: 0.08,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 32768,
            max_tokens: 16384,
            headers: None,
            compat: None,
        },
        Model {
            id: "mistralai/mistral-small-3.1-24b-instruct".into(),
            name: "Mistral: Mistral Small 3.1 24B".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.03,
                output: 0.11,
                cache_read: 0.015,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 131072,
            headers: None,
            compat: None,
        },
        Model {
            id: "mistralai/mistral-small-3.1-24b-instruct:free".into(),
            name: "Mistral: Mistral Small 3.1 24B (free)".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.0,
                output: 0.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "mistralai/mistral-small-3.2-24b-instruct".into(),
            name: "Mistral: Mistral Small 3.2 24B".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.06,
                output: 0.18,
                cache_read: 0.03,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 131072,
            headers: None,
            compat: None,
        },
        Model {
            id: "mistralai/mistral-small-creative".into(),
            name: "Mistral: Mistral Small Creative".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.09999999999999999,
                output: 0.3,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 32768,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "mistralai/mistral-tiny".into(),
            name: "Mistral Tiny".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.25,
                output: 0.25,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 32768,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "mistralai/mixtral-8x22b-instruct".into(),
            name: "Mistral: Mixtral 8x22B Instruct".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 2.0,
                output: 6.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 65536,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "mistralai/mixtral-8x7b-instruct".into(),
            name: "Mistral: Mixtral 8x7B Instruct".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.54,
                output: 0.54,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 32768,
            max_tokens: 16384,
            headers: None,
            compat: None,
        },
        Model {
            id: "mistralai/pixtral-12b".into(),
            name: "Mistral: Pixtral 12B".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.09999999999999999,
                output: 0.09999999999999999,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 32768,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "mistralai/pixtral-large-2411".into(),
            name: "Mistral: Pixtral Large 2411".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 2.0,
                output: 6.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "mistralai/voxtral-small-24b-2507".into(),
            name: "Mistral: Voxtral Small 24B 2507".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.09999999999999999,
                output: 0.3,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 32000,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "moonshotai/kimi-k2".into(),
            name: "MoonshotAI: Kimi K2 0711".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.5,
                output: 2.4,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "moonshotai/kimi-k2-0905".into(),
            name: "MoonshotAI: Kimi K2 0905".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.39,
                output: 1.9,
                cache_read: 0.195,
                cache_write: 0.0,
            },
            context_window: 262144,
            max_tokens: 262144,
            headers: None,
            compat: None,
        },
        Model {
            id: "moonshotai/kimi-k2-0905:exacto".into(),
            name: "MoonshotAI: Kimi K2 0905 (exacto)".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.6,
                output: 2.5,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 262144,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "moonshotai/kimi-k2-thinking".into(),
            name: "MoonshotAI: Kimi K2 Thinking".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.39999999999999997,
                output: 1.75,
                cache_read: 0.19999999999999998,
                cache_write: 0.0,
            },
            context_window: 262144,
            max_tokens: 65535,
            headers: None,
            compat: None,
        },
        Model {
            id: "moonshotai/kimi-k2.5".into(),
            name: "MoonshotAI: Kimi K2.5".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.3,
                output: 1.5,
                cache_read: 0.049999999999999996,
                cache_write: 0.0,
            },
            context_window: 262144,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "nex-agi/deepseek-v3.1-nex-n1".into(),
            name: "Nex AGI: DeepSeek V3.1 Nex N1".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.27,
                output: 1.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 163840,
            headers: None,
            compat: None,
        },
        Model {
            id: "nousresearch/deephermes-3-mistral-24b-preview".into(),
            name: "Nous: DeepHermes 3 Mistral 24B Preview".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.02,
                output: 0.09999999999999999,
                cache_read: 0.01,
                cache_write: 0.0,
            },
            context_window: 32768,
            max_tokens: 32768,
            headers: None,
            compat: None,
        },
        Model {
            id: "nousresearch/hermes-4-70b".into(),
            name: "Nous: Hermes 4 70B".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.11,
                output: 0.38,
                cache_read: 0.055,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 131072,
            headers: None,
            compat: None,
        },
        Model {
            id: "nvidia/llama-3.1-nemotron-70b-instruct".into(),
            name: "NVIDIA: Llama 3.1 Nemotron 70B Instruct".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 1.2,
                output: 1.2,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 16384,
            headers: None,
            compat: None,
        },
        Model {
            id: "nvidia/llama-3.3-nemotron-super-49b-v1.5".into(),
            name: "NVIDIA: Llama 3.3 Nemotron Super 49B V1.5".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.09999999999999999,
                output: 0.39999999999999997,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "nvidia/nemotron-3-nano-30b-a3b".into(),
            name: "NVIDIA: Nemotron 3 Nano 30B A3B".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.049999999999999996,
                output: 0.19999999999999998,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 262144,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "nvidia/nemotron-3-nano-30b-a3b:free".into(),
            name: "NVIDIA: Nemotron 3 Nano 30B A3B (free)".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.0,
                output: 0.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 256000,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "nvidia/nemotron-nano-12b-v2-vl:free".into(),
            name: "NVIDIA: Nemotron Nano 12B 2 VL (free)".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.0,
                output: 0.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "nvidia/nemotron-nano-9b-v2".into(),
            name: "NVIDIA: Nemotron Nano 9B V2".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.04,
                output: 0.16,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "nvidia/nemotron-nano-9b-v2:free".into(),
            name: "NVIDIA: Nemotron Nano 9B V2 (free)".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.0,
                output: 0.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/gpt-3.5-turbo".into(),
            name: "OpenAI: GPT-3.5 Turbo".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.5,
                output: 1.5,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 16385,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/gpt-3.5-turbo-0613".into(),
            name: "OpenAI: GPT-3.5 Turbo (older v0613)".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 1.0,
                output: 2.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 4095,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/gpt-3.5-turbo-16k".into(),
            name: "OpenAI: GPT-3.5 Turbo 16k".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 3.0,
                output: 4.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 16385,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/gpt-4".into(),
            name: "OpenAI: GPT-4".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 30.0,
                output: 60.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 8191,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/gpt-4-0314".into(),
            name: "OpenAI: GPT-4 (older v0314)".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 30.0,
                output: 60.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 8191,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/gpt-4-1106-preview".into(),
            name: "OpenAI: GPT-4 Turbo (older v1106)".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 10.0,
                output: 30.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/gpt-4-turbo".into(),
            name: "OpenAI: GPT-4 Turbo".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 10.0,
                output: 30.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/gpt-4-turbo-preview".into(),
            name: "OpenAI: GPT-4 Turbo Preview".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 10.0,
                output: 30.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/gpt-4.1".into(),
            name: "OpenAI: GPT-4.1".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 2.0,
                output: 8.0,
                cache_read: 0.5,
                cache_write: 0.0,
            },
            context_window: 1047576,
            max_tokens: 32768,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/gpt-4.1-mini".into(),
            name: "OpenAI: GPT-4.1 Mini".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.39999999999999997,
                output: 1.5999999999999999,
                cache_read: 0.09999999999999999,
                cache_write: 0.0,
            },
            context_window: 1047576,
            max_tokens: 32768,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/gpt-4.1-nano".into(),
            name: "OpenAI: GPT-4.1 Nano".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.09999999999999999,
                output: 0.39999999999999997,
                cache_read: 0.024999999999999998,
                cache_write: 0.0,
            },
            context_window: 1047576,
            max_tokens: 32768,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/gpt-4o".into(),
            name: "OpenAI: GPT-4o".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 2.5,
                output: 10.0,
                cache_read: 1.25,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 16384,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/gpt-4o-2024-05-13".into(),
            name: "OpenAI: GPT-4o (2024-05-13)".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 5.0,
                output: 15.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/gpt-4o-2024-08-06".into(),
            name: "OpenAI: GPT-4o (2024-08-06)".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 2.5,
                output: 10.0,
                cache_read: 1.25,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 16384,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/gpt-4o-2024-11-20".into(),
            name: "OpenAI: GPT-4o (2024-11-20)".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 2.5,
                output: 10.0,
                cache_read: 1.25,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 16384,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/gpt-4o-audio-preview".into(),
            name: "OpenAI: GPT-4o Audio".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 2.5,
                output: 10.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 16384,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/gpt-4o-mini".into(),
            name: "OpenAI: GPT-4o-mini".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.15,
                output: 0.6,
                cache_read: 0.075,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 16384,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/gpt-4o-mini-2024-07-18".into(),
            name: "OpenAI: GPT-4o-mini (2024-07-18)".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.15,
                output: 0.6,
                cache_read: 0.075,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 16384,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/gpt-4o:extended".into(),
            name: "OpenAI: GPT-4o (extended)".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 6.0,
                output: 18.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 64000,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/gpt-5".into(),
            name: "OpenAI: GPT-5".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.25,
                output: 10.0,
                cache_read: 0.125,
                cache_write: 0.0,
            },
            context_window: 400000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/gpt-5-codex".into(),
            name: "OpenAI: GPT-5 Codex".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.25,
                output: 10.0,
                cache_read: 0.125,
                cache_write: 0.0,
            },
            context_window: 400000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/gpt-5-image".into(),
            name: "OpenAI: GPT-5 Image".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 10.0,
                output: 10.0,
                cache_read: 1.25,
                cache_write: 0.0,
            },
            context_window: 400000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/gpt-5-image-mini".into(),
            name: "OpenAI: GPT-5 Image Mini".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 2.5,
                output: 2.0,
                cache_read: 0.25,
                cache_write: 0.0,
            },
            context_window: 400000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/gpt-5-mini".into(),
            name: "OpenAI: GPT-5 Mini".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.25,
                output: 2.0,
                cache_read: 0.024999999999999998,
                cache_write: 0.0,
            },
            context_window: 400000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/gpt-5-nano".into(),
            name: "OpenAI: GPT-5 Nano".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.049999999999999996,
                output: 0.39999999999999997,
                cache_read: 0.005,
                cache_write: 0.0,
            },
            context_window: 400000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/gpt-5-pro".into(),
            name: "OpenAI: GPT-5 Pro".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 15.0,
                output: 120.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 400000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/gpt-5.1".into(),
            name: "OpenAI: GPT-5.1".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.25,
                output: 10.0,
                cache_read: 0.125,
                cache_write: 0.0,
            },
            context_window: 400000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/gpt-5.1-chat".into(),
            name: "OpenAI: GPT-5.1 Chat".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.25,
                output: 10.0,
                cache_read: 0.125,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 16384,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/gpt-5.1-codex".into(),
            name: "OpenAI: GPT-5.1-Codex".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.25,
                output: 10.0,
                cache_read: 0.125,
                cache_write: 0.0,
            },
            context_window: 400000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/gpt-5.1-codex-max".into(),
            name: "OpenAI: GPT-5.1-Codex-Max".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.25,
                output: 10.0,
                cache_read: 0.125,
                cache_write: 0.0,
            },
            context_window: 400000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/gpt-5.1-codex-mini".into(),
            name: "OpenAI: GPT-5.1-Codex-Mini".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.25,
                output: 2.0,
                cache_read: 0.024999999999999998,
                cache_write: 0.0,
            },
            context_window: 400000,
            max_tokens: 100000,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/gpt-5.2".into(),
            name: "OpenAI: GPT-5.2".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.75,
                output: 14.0,
                cache_read: 0.175,
                cache_write: 0.0,
            },
            context_window: 400000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/gpt-5.2-chat".into(),
            name: "OpenAI: GPT-5.2 Chat".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.75,
                output: 14.0,
                cache_read: 0.175,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 16384,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/gpt-5.2-codex".into(),
            name: "OpenAI: GPT-5.2-Codex".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.75,
                output: 14.0,
                cache_read: 0.175,
                cache_write: 0.0,
            },
            context_window: 400000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/gpt-5.2-pro".into(),
            name: "OpenAI: GPT-5.2 Pro".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 21.0,
                output: 168.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 400000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/gpt-oss-120b".into(),
            name: "OpenAI: gpt-oss-120b".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.039,
                output: 0.19,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/gpt-oss-120b:exacto".into(),
            name: "OpenAI: gpt-oss-120b (exacto)".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.039,
                output: 0.19,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/gpt-oss-120b:free".into(),
            name: "OpenAI: gpt-oss-120b (free)".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.0,
                output: 0.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 131072,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/gpt-oss-20b".into(),
            name: "OpenAI: gpt-oss-20b".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.03,
                output: 0.14,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/gpt-oss-20b:free".into(),
            name: "OpenAI: gpt-oss-20b (free)".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.0,
                output: 0.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 131072,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/gpt-oss-safeguard-20b".into(),
            name: "OpenAI: gpt-oss-safeguard-20b".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.075,
                output: 0.3,
                cache_read: 0.037,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 65536,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/o1".into(),
            name: "OpenAI: o1".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 15.0,
                output: 60.0,
                cache_read: 7.5,
                cache_write: 0.0,
            },
            context_window: 200000,
            max_tokens: 100000,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/o3".into(),
            name: "OpenAI: o3".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 2.0,
                output: 8.0,
                cache_read: 0.5,
                cache_write: 0.0,
            },
            context_window: 200000,
            max_tokens: 100000,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/o3-deep-research".into(),
            name: "OpenAI: o3 Deep Research".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 10.0,
                output: 40.0,
                cache_read: 2.5,
                cache_write: 0.0,
            },
            context_window: 200000,
            max_tokens: 100000,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/o3-mini".into(),
            name: "OpenAI: o3 Mini".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 1.1,
                output: 4.4,
                cache_read: 0.55,
                cache_write: 0.0,
            },
            context_window: 200000,
            max_tokens: 100000,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/o3-mini-high".into(),
            name: "OpenAI: o3 Mini High".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 1.1,
                output: 4.4,
                cache_read: 0.55,
                cache_write: 0.0,
            },
            context_window: 200000,
            max_tokens: 100000,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/o3-pro".into(),
            name: "OpenAI: o3 Pro".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 20.0,
                output: 80.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 200000,
            max_tokens: 100000,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/o4-mini".into(),
            name: "OpenAI: o4 Mini".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.1,
                output: 4.4,
                cache_read: 0.275,
                cache_write: 0.0,
            },
            context_window: 200000,
            max_tokens: 100000,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/o4-mini-deep-research".into(),
            name: "OpenAI: o4 Mini Deep Research".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 2.0,
                output: 8.0,
                cache_read: 0.5,
                cache_write: 0.0,
            },
            context_window: 200000,
            max_tokens: 100000,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/o4-mini-high".into(),
            name: "OpenAI: o4 Mini High".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.1,
                output: 4.4,
                cache_read: 0.275,
                cache_write: 0.0,
            },
            context_window: 200000,
            max_tokens: 100000,
            headers: None,
            compat: None,
        },
        Model {
            id: "openrouter/auto".into(),
            name: "Auto Router".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: -1000000.0,
                output: -1000000.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 2000000,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "openrouter/free".into(),
            name: "Free Models Router".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.0,
                output: 0.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 200000,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "openrouter/pony-alpha".into(),
            name: "Pony Alpha".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.0,
                output: 0.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 200000,
            max_tokens: 131000,
            headers: None,
            compat: None,
        },
        Model {
            id: "prime-intellect/intellect-3".into(),
            name: "Prime Intellect: INTELLECT-3".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.19999999999999998,
                output: 1.1,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 131072,
            headers: None,
            compat: None,
        },
        Model {
            id: "qwen/qwen-2.5-72b-instruct".into(),
            name: "Qwen2.5 72B Instruct".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.12,
                output: 0.39,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 32768,
            max_tokens: 16384,
            headers: None,
            compat: None,
        },
        Model {
            id: "qwen/qwen-2.5-7b-instruct".into(),
            name: "Qwen: Qwen2.5 7B Instruct".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.04,
                output: 0.09999999999999999,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 32768,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "qwen/qwen-max".into(),
            name: "Qwen: Qwen-Max ".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 1.5999999999999999,
                output: 6.3999999999999995,
                cache_read: 0.64,
                cache_write: 0.0,
            },
            context_window: 32768,
            max_tokens: 8192,
            headers: None,
            compat: None,
        },
        Model {
            id: "qwen/qwen-plus".into(),
            name: "Qwen: Qwen-Plus".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.39999999999999997,
                output: 1.2,
                cache_read: 0.16,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 8192,
            headers: None,
            compat: None,
        },
        Model {
            id: "qwen/qwen-plus-2025-07-28".into(),
            name: "Qwen: Qwen Plus 0728".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.39999999999999997,
                output: 1.2,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 1000000,
            max_tokens: 32768,
            headers: None,
            compat: None,
        },
        Model {
            id: "qwen/qwen-plus-2025-07-28:thinking".into(),
            name: "Qwen: Qwen Plus 0728 (thinking)".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.39999999999999997,
                output: 4.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 1000000,
            max_tokens: 32768,
            headers: None,
            compat: None,
        },
        Model {
            id: "qwen/qwen-turbo".into(),
            name: "Qwen: Qwen-Turbo".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.049999999999999996,
                output: 0.19999999999999998,
                cache_read: 0.02,
                cache_write: 0.0,
            },
            context_window: 1000000,
            max_tokens: 8192,
            headers: None,
            compat: None,
        },
        Model {
            id: "qwen/qwen-vl-max".into(),
            name: "Qwen: Qwen VL Max".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.7999999999999999,
                output: 3.1999999999999997,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 8192,
            headers: None,
            compat: None,
        },
        Model {
            id: "qwen/qwen3-14b".into(),
            name: "Qwen: Qwen3 14B".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.049999999999999996,
                output: 0.22,
                cache_read: 0.024999999999999998,
                cache_write: 0.0,
            },
            context_window: 40960,
            max_tokens: 40960,
            headers: None,
            compat: None,
        },
        Model {
            id: "qwen/qwen3-235b-a22b".into(),
            name: "Qwen: Qwen3 235B A22B".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.19999999999999998,
                output: 0.6,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 40960,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "qwen/qwen3-235b-a22b-2507".into(),
            name: "Qwen: Qwen3 235B A22B Instruct 2507".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.071,
                output: 0.09999999999999999,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 262144,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "qwen/qwen3-235b-a22b-thinking-2507".into(),
            name: "Qwen: Qwen3 235B A22B Thinking 2507".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.11,
                output: 0.6,
                cache_read: 0.055,
                cache_write: 0.0,
            },
            context_window: 262144,
            max_tokens: 262144,
            headers: None,
            compat: None,
        },
        Model {
            id: "qwen/qwen3-30b-a3b".into(),
            name: "Qwen: Qwen3 30B A3B".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.06,
                output: 0.22,
                cache_read: 0.03,
                cache_write: 0.0,
            },
            context_window: 40960,
            max_tokens: 40960,
            headers: None,
            compat: None,
        },
        Model {
            id: "qwen/qwen3-30b-a3b-instruct-2507".into(),
            name: "Qwen: Qwen3 30B A3B Instruct 2507".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.08,
                output: 0.33,
                cache_read: 0.04,
                cache_write: 0.0,
            },
            context_window: 262144,
            max_tokens: 262144,
            headers: None,
            compat: None,
        },
        Model {
            id: "qwen/qwen3-30b-a3b-thinking-2507".into(),
            name: "Qwen: Qwen3 30B A3B Thinking 2507".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.051,
                output: 0.33999999999999997,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 32768,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "qwen/qwen3-32b".into(),
            name: "Qwen: Qwen3 32B".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.08,
                output: 0.24,
                cache_read: 0.04,
                cache_write: 0.0,
            },
            context_window: 40960,
            max_tokens: 40960,
            headers: None,
            compat: None,
        },
        Model {
            id: "qwen/qwen3-4b:free".into(),
            name: "Qwen: Qwen3 4B (free)".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.0,
                output: 0.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 40960,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "qwen/qwen3-8b".into(),
            name: "Qwen: Qwen3 8B".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.049999999999999996,
                output: 0.39999999999999997,
                cache_read: 0.049999999999999996,
                cache_write: 0.0,
            },
            context_window: 32000,
            max_tokens: 8192,
            headers: None,
            compat: None,
        },
        Model {
            id: "qwen/qwen3-coder".into(),
            name: "Qwen: Qwen3 Coder 480B A35B".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.22,
                output: 1.0,
                cache_read: 0.022,
                cache_write: 0.0,
            },
            context_window: 262144,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "qwen/qwen3-coder-30b-a3b-instruct".into(),
            name: "Qwen: Qwen3 Coder 30B A3B Instruct".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.07,
                output: 0.27,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 160000,
            max_tokens: 32768,
            headers: None,
            compat: None,
        },
        Model {
            id: "qwen/qwen3-coder-flash".into(),
            name: "Qwen: Qwen3 Coder Flash".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.3,
                output: 1.5,
                cache_read: 0.08,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 65536,
            headers: None,
            compat: None,
        },
        Model {
            id: "qwen/qwen3-coder-next".into(),
            name: "Qwen: Qwen3 Coder Next".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.07,
                output: 0.3,
                cache_read: 0.035,
                cache_write: 0.0,
            },
            context_window: 262144,
            max_tokens: 65536,
            headers: None,
            compat: None,
        },
        Model {
            id: "qwen/qwen3-coder-plus".into(),
            name: "Qwen: Qwen3 Coder Plus".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 1.0,
                output: 5.0,
                cache_read: 0.09999999999999999,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 65536,
            headers: None,
            compat: None,
        },
        Model {
            id: "qwen/qwen3-coder:exacto".into(),
            name: "Qwen: Qwen3 Coder 480B A35B (exacto)".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.22,
                output: 1.7999999999999998,
                cache_read: 0.022,
                cache_write: 0.0,
            },
            context_window: 262144,
            max_tokens: 65536,
            headers: None,
            compat: None,
        },
        Model {
            id: "qwen/qwen3-coder:free".into(),
            name: "Qwen: Qwen3 Coder 480B A35B (free)".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.0,
                output: 0.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 262000,
            max_tokens: 262000,
            headers: None,
            compat: None,
        },
        Model {
            id: "qwen/qwen3-max".into(),
            name: "Qwen: Qwen3 Max".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 1.2,
                output: 6.0,
                cache_read: 0.24,
                cache_write: 0.0,
            },
            context_window: 256000,
            max_tokens: 32768,
            headers: None,
            compat: None,
        },
        Model {
            id: "qwen/qwen3-next-80b-a3b-instruct".into(),
            name: "Qwen: Qwen3 Next 80B A3B Instruct".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.09,
                output: 1.1,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 262144,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "qwen/qwen3-next-80b-a3b-instruct:free".into(),
            name: "Qwen: Qwen3 Next 80B A3B Instruct (free)".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.0,
                output: 0.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 262144,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "qwen/qwen3-next-80b-a3b-thinking".into(),
            name: "Qwen: Qwen3 Next 80B A3B Thinking".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.15,
                output: 1.2,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "qwen/qwen3-vl-235b-a22b-instruct".into(),
            name: "Qwen: Qwen3 VL 235B A22B Instruct".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.19999999999999998,
                output: 0.88,
                cache_read: 0.11,
                cache_write: 0.0,
            },
            context_window: 262144,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "qwen/qwen3-vl-235b-a22b-thinking".into(),
            name: "Qwen: Qwen3 VL 235B A22B Thinking".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.44999999999999996,
                output: 3.5,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 262144,
            max_tokens: 262144,
            headers: None,
            compat: None,
        },
        Model {
            id: "qwen/qwen3-vl-30b-a3b-instruct".into(),
            name: "Qwen: Qwen3 VL 30B A3B Instruct".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.15,
                output: 0.6,
                cache_read: 0.075,
                cache_write: 0.0,
            },
            context_window: 262144,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "qwen/qwen3-vl-30b-a3b-thinking".into(),
            name: "Qwen: Qwen3 VL 30B A3B Thinking".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.19999999999999998,
                output: 1.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 32768,
            headers: None,
            compat: None,
        },
        Model {
            id: "qwen/qwen3-vl-8b-instruct".into(),
            name: "Qwen: Qwen3 VL 8B Instruct".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.08,
                output: 0.5,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 32768,
            headers: None,
            compat: None,
        },
        Model {
            id: "qwen/qwen3-vl-8b-thinking".into(),
            name: "Qwen: Qwen3 VL 8B Thinking".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.18,
                output: 2.0999999999999996,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 256000,
            max_tokens: 32768,
            headers: None,
            compat: None,
        },
        Model {
            id: "qwen/qwq-32b".into(),
            name: "Qwen: QwQ 32B".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.15,
                output: 0.39999999999999997,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 32768,
            max_tokens: 32768,
            headers: None,
            compat: None,
        },
        Model {
            id: "relace/relace-search".into(),
            name: "Relace: Relace Search".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 1.0,
                output: 3.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 256000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "sao10k/l3-euryale-70b".into(),
            name: "Sao10k: Llama 3 Euryale 70B v2.1".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 1.48,
                output: 1.48,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 8192,
            max_tokens: 8192,
            headers: None,
            compat: None,
        },
        Model {
            id: "sao10k/l3.1-euryale-70b".into(),
            name: "Sao10K: Llama 3.1 Euryale 70B v2.2".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.65,
                output: 0.75,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 32768,
            max_tokens: 32768,
            headers: None,
            compat: None,
        },
        Model {
            id: "stepfun-ai/step3".into(),
            name: "StepFun: Step3".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.5700000000000001,
                output: 1.42,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 65536,
            max_tokens: 65536,
            headers: None,
            compat: None,
        },
        Model {
            id: "stepfun/step-3.5-flash:free".into(),
            name: "StepFun: Step 3.5 Flash (free)".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.0,
                output: 0.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 256000,
            max_tokens: 256000,
            headers: None,
            compat: None,
        },
        Model {
            id: "thedrummer/rocinante-12b".into(),
            name: "TheDrummer: Rocinante 12B".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.16999999999999998,
                output: 0.43,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 32768,
            max_tokens: 32768,
            headers: None,
            compat: None,
        },
        Model {
            id: "thedrummer/unslopnemo-12b".into(),
            name: "TheDrummer: UnslopNemo 12B".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.39999999999999997,
                output: 0.39999999999999997,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 32768,
            max_tokens: 32768,
            headers: None,
            compat: None,
        },
        Model {
            id: "tngtech/deepseek-r1t2-chimera".into(),
            name: "TNG: DeepSeek R1T2 Chimera".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.25,
                output: 0.85,
                cache_read: 0.125,
                cache_write: 0.0,
            },
            context_window: 163840,
            max_tokens: 163840,
            headers: None,
            compat: None,
        },
        Model {
            id: "tngtech/tng-r1t-chimera".into(),
            name: "TNG: R1T Chimera".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.25,
                output: 0.85,
                cache_read: 0.125,
                cache_write: 0.0,
            },
            context_window: 163840,
            max_tokens: 65536,
            headers: None,
            compat: None,
        },
        Model {
            id: "tngtech/tng-r1t-chimera:free".into(),
            name: "TNG: R1T Chimera (free)".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.0,
                output: 0.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 163840,
            max_tokens: 65536,
            headers: None,
            compat: None,
        },
        Model {
            id: "upstage/solar-pro-3:free".into(),
            name: "Upstage: Solar Pro 3 (free)".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.0,
                output: 0.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "x-ai/grok-3".into(),
            name: "xAI: Grok 3".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 3.0,
                output: 15.0,
                cache_read: 0.75,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "x-ai/grok-3-beta".into(),
            name: "xAI: Grok 3 Beta".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 3.0,
                output: 15.0,
                cache_read: 0.75,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "x-ai/grok-3-mini".into(),
            name: "xAI: Grok 3 Mini".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.3,
                output: 0.5,
                cache_read: 0.075,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "x-ai/grok-3-mini-beta".into(),
            name: "xAI: Grok 3 Mini Beta".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.3,
                output: 0.5,
                cache_read: 0.075,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "x-ai/grok-4".into(),
            name: "xAI: Grok 4".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 3.0,
                output: 15.0,
                cache_read: 0.75,
                cache_write: 0.0,
            },
            context_window: 256000,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "x-ai/grok-4-fast".into(),
            name: "xAI: Grok 4 Fast".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.19999999999999998,
                output: 0.5,
                cache_read: 0.049999999999999996,
                cache_write: 0.0,
            },
            context_window: 2000000,
            max_tokens: 30000,
            headers: None,
            compat: None,
        },
        Model {
            id: "x-ai/grok-4.1-fast".into(),
            name: "xAI: Grok 4.1 Fast".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.19999999999999998,
                output: 0.5,
                cache_read: 0.049999999999999996,
                cache_write: 0.0,
            },
            context_window: 2000000,
            max_tokens: 30000,
            headers: None,
            compat: None,
        },
        Model {
            id: "x-ai/grok-code-fast-1".into(),
            name: "xAI: Grok Code Fast 1".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.19999999999999998,
                output: 1.5,
                cache_read: 0.02,
                cache_write: 0.0,
            },
            context_window: 256000,
            max_tokens: 10000,
            headers: None,
            compat: None,
        },
        Model {
            id: "xiaomi/mimo-v2-flash".into(),
            name: "Xiaomi: MiMo-V2-Flash".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.09,
                output: 0.29,
                cache_read: 0.045,
                cache_write: 0.0,
            },
            context_window: 262144,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "z-ai/glm-4-32b".into(),
            name: "Z.AI: GLM 4 32B ".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.09999999999999999,
                output: 0.09999999999999999,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "z-ai/glm-4.5".into(),
            name: "Z.AI: GLM 4.5".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.35,
                output: 1.55,
                cache_read: 0.175,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 65536,
            headers: None,
            compat: None,
        },
        Model {
            id: "z-ai/glm-4.5-air".into(),
            name: "Z.AI: GLM 4.5 Air".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.13,
                output: 0.85,
                cache_read: 0.024999999999999998,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 98304,
            headers: None,
            compat: None,
        },
        Model {
            id: "z-ai/glm-4.5-air:free".into(),
            name: "Z.AI: GLM 4.5 Air (free)".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.0,
                output: 0.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 96000,
            headers: None,
            compat: None,
        },
        Model {
            id: "z-ai/glm-4.5v".into(),
            name: "Z.AI: GLM 4.5V".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.6,
                output: 1.7999999999999998,
                cache_read: 0.11,
                cache_write: 0.0,
            },
            context_window: 65536,
            max_tokens: 16384,
            headers: None,
            compat: None,
        },
        Model {
            id: "z-ai/glm-4.6".into(),
            name: "Z.AI: GLM 4.6".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.35,
                output: 1.5,
                cache_read: 0.175,
                cache_write: 0.0,
            },
            context_window: 202752,
            max_tokens: 65536,
            headers: None,
            compat: None,
        },
        Model {
            id: "z-ai/glm-4.6:exacto".into(),
            name: "Z.AI: GLM 4.6 (exacto)".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.44,
                output: 1.76,
                cache_read: 0.11,
                cache_write: 0.0,
            },
            context_window: 204800,
            max_tokens: 131072,
            headers: None,
            compat: None,
        },
        Model {
            id: "z-ai/glm-4.6v".into(),
            name: "Z.AI: GLM 4.6V".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.3,
                output: 0.8999999999999999,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 131072,
            headers: None,
            compat: None,
        },
        Model {
            id: "z-ai/glm-4.7".into(),
            name: "Z.AI: GLM 4.7".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.39999999999999997,
                output: 1.5,
                cache_read: 0.19999999999999998,
                cache_write: 0.0,
            },
            context_window: 202752,
            max_tokens: 65535,
            headers: None,
            compat: None,
        },
        Model {
            id: "z-ai/glm-4.7-flash".into(),
            name: "Z.AI: GLM 4.7 Flash".into(),
            api: "openai-completions".into(),
            provider: "openrouter".into(),
            base_url: "https://openrouter.ai/api/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.06,
                output: 0.39999999999999997,
                cache_read: 0.0100000002,
                cache_write: 0.0,
            },
            context_window: 202752,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "alibaba/qwen-3-14b".into(),
            name: "Qwen3-14B".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.06,
                output: 0.24,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 40960,
            max_tokens: 16384,
            headers: None,
            compat: None,
        },
        Model {
            id: "alibaba/qwen-3-235b".into(),
            name: "Qwen3-235B-A22B".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.071,
                output: 0.463,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 40960,
            max_tokens: 16384,
            headers: None,
            compat: None,
        },
        Model {
            id: "alibaba/qwen-3-30b".into(),
            name: "Qwen3-30B-A3B".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.08,
                output: 0.29,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 40960,
            max_tokens: 16384,
            headers: None,
            compat: None,
        },
        Model {
            id: "alibaba/qwen-3-32b".into(),
            name: "Qwen 3 32B".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.09999999999999999,
                output: 0.3,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 40960,
            max_tokens: 16384,
            headers: None,
            compat: None,
        },
        Model {
            id: "alibaba/qwen3-235b-a22b-thinking".into(),
            name: "Qwen3 235B A22B Thinking 2507".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.3,
                output: 2.9000000000000004,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 262114,
            max_tokens: 262114,
            headers: None,
            compat: None,
        },
        Model {
            id: "alibaba/qwen3-coder".into(),
            name: "Qwen3 Coder 480B A35B Instruct".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.39999999999999997,
                output: 1.5999999999999999,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 262144,
            max_tokens: 66536,
            headers: None,
            compat: None,
        },
        Model {
            id: "alibaba/qwen3-coder-30b-a3b".into(),
            name: "Qwen 3 Coder 30B A3B Instruct".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.07,
                output: 0.27,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 160000,
            max_tokens: 32768,
            headers: None,
            compat: None,
        },
        Model {
            id: "alibaba/qwen3-coder-next".into(),
            name: "Qwen3 Coder Next".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.5,
                output: 1.2,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 256000,
            max_tokens: 256000,
            headers: None,
            compat: None,
        },
        Model {
            id: "alibaba/qwen3-coder-plus".into(),
            name: "Qwen3 Coder Plus".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 1.0,
                output: 5.0,
                cache_read: 0.19999999999999998,
                cache_write: 0.0,
            },
            context_window: 1000000,
            max_tokens: 65536,
            headers: None,
            compat: None,
        },
        Model {
            id: "alibaba/qwen3-max-preview".into(),
            name: "Qwen3 Max Preview".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 1.2,
                output: 6.0,
                cache_read: 0.24,
                cache_write: 0.0,
            },
            context_window: 262144,
            max_tokens: 32768,
            headers: None,
            compat: None,
        },
        Model {
            id: "alibaba/qwen3-max-thinking".into(),
            name: "Qwen 3 Max Thinking".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 1.2,
                output: 6.0,
                cache_read: 0.24,
                cache_write: 0.0,
            },
            context_window: 256000,
            max_tokens: 65536,
            headers: None,
            compat: None,
        },
        Model {
            id: "alibaba/qwen3-vl-thinking".into(),
            name: "Qwen3 VL 235B A22B Thinking".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.22,
                output: 0.88,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 256000,
            max_tokens: 256000,
            headers: None,
            compat: None,
        },
        Model {
            id: "anthropic/claude-3-haiku".into(),
            name: "Claude 3 Haiku".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.25,
                output: 1.25,
                cache_read: 0.03,
                cache_write: 0.3,
            },
            context_window: 200000,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "anthropic/claude-3.5-haiku".into(),
            name: "Claude 3.5 Haiku".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.7999999999999999,
                output: 4.0,
                cache_read: 0.08,
                cache_write: 1.0,
            },
            context_window: 200000,
            max_tokens: 8192,
            headers: None,
            compat: None,
        },
        Model {
            id: "anthropic/claude-3.5-sonnet".into(),
            name: "Claude 3.5 Sonnet".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 3.0,
                output: 15.0,
                cache_read: 0.3,
                cache_write: 3.75,
            },
            context_window: 200000,
            max_tokens: 8192,
            headers: None,
            compat: None,
        },
        Model {
            id: "anthropic/claude-3.5-sonnet-20240620".into(),
            name: "Claude 3.5 Sonnet (2024-06-20)".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 3.0,
                output: 15.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 200000,
            max_tokens: 8192,
            headers: None,
            compat: None,
        },
        Model {
            id: "anthropic/claude-3.7-sonnet".into(),
            name: "Claude 3.7 Sonnet".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 3.0,
                output: 15.0,
                cache_read: 0.3,
                cache_write: 3.75,
            },
            context_window: 200000,
            max_tokens: 64000,
            headers: None,
            compat: None,
        },
        Model {
            id: "anthropic/claude-haiku-4.5".into(),
            name: "Claude Haiku 4.5".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.0,
                output: 5.0,
                cache_read: 0.09999999999999999,
                cache_write: 1.25,
            },
            context_window: 200000,
            max_tokens: 64000,
            headers: None,
            compat: None,
        },
        Model {
            id: "anthropic/claude-opus-4".into(),
            name: "Claude Opus 4".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 15.0,
                output: 75.0,
                cache_read: 1.5,
                cache_write: 18.75,
            },
            context_window: 200000,
            max_tokens: 32000,
            headers: None,
            compat: None,
        },
        Model {
            id: "anthropic/claude-opus-4.1".into(),
            name: "Claude Opus 4.1".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 15.0,
                output: 75.0,
                cache_read: 1.5,
                cache_write: 18.75,
            },
            context_window: 200000,
            max_tokens: 32000,
            headers: None,
            compat: None,
        },
        Model {
            id: "anthropic/claude-opus-4.5".into(),
            name: "Claude Opus 4.5".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 5.0,
                output: 25.0,
                cache_read: 0.5,
                cache_write: 6.25,
            },
            context_window: 200000,
            max_tokens: 64000,
            headers: None,
            compat: None,
        },
        Model {
            id: "anthropic/claude-opus-4.6".into(),
            name: "Claude Opus 4.6".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 5.0,
                output: 25.0,
                cache_read: 0.5,
                cache_write: 6.25,
            },
            context_window: 1000000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "anthropic/claude-sonnet-4".into(),
            name: "Claude Sonnet 4".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 3.0,
                output: 15.0,
                cache_read: 0.3,
                cache_write: 3.75,
            },
            context_window: 1000000,
            max_tokens: 64000,
            headers: None,
            compat: None,
        },
        Model {
            id: "anthropic/claude-sonnet-4.5".into(),
            name: "Claude Sonnet 4.5".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 3.0,
                output: 15.0,
                cache_read: 0.3,
                cache_write: 3.75,
            },
            context_window: 1000000,
            max_tokens: 64000,
            headers: None,
            compat: None,
        },
        Model {
            id: "arcee-ai/trinity-large-preview".into(),
            name: "Trinity Large Preview".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.25,
                output: 1.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131000,
            max_tokens: 131000,
            headers: None,
            compat: None,
        },
        Model {
            id: "bytedance/seed-1.6".into(),
            name: "Seed 1.6".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.25,
                output: 2.0,
                cache_read: 0.049999999999999996,
                cache_write: 0.0,
            },
            context_window: 256000,
            max_tokens: 32000,
            headers: None,
            compat: None,
        },
        Model {
            id: "cohere/command-a".into(),
            name: "Command A".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 2.5,
                output: 10.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 256000,
            max_tokens: 8000,
            headers: None,
            compat: None,
        },
        Model {
            id: "deepseek/deepseek-v3".into(),
            name: "DeepSeek V3 0324".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.77,
                output: 0.77,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 163840,
            max_tokens: 16384,
            headers: None,
            compat: None,
        },
        Model {
            id: "deepseek/deepseek-v3.1".into(),
            name: "DeepSeek-V3.1".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.3,
                output: 1.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 163840,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "deepseek/deepseek-v3.1-terminus".into(),
            name: "DeepSeek V3.1 Terminus".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.27,
                output: 1.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 65536,
            headers: None,
            compat: None,
        },
        Model {
            id: "deepseek/deepseek-v3.2-exp".into(),
            name: "DeepSeek V3.2 Exp".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.27,
                output: 0.39999999999999997,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 163840,
            max_tokens: 163840,
            headers: None,
            compat: None,
        },
        Model {
            id: "deepseek/deepseek-v3.2-thinking".into(),
            name: "DeepSeek V3.2 Thinking".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.28,
                output: 0.42,
                cache_read: 0.028,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 64000,
            headers: None,
            compat: None,
        },
        Model {
            id: "google/gemini-2.5-flash".into(),
            name: "Gemini 2.5 Flash".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.3,
                output: 2.5,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 1000000,
            max_tokens: 65536,
            headers: None,
            compat: None,
        },
        Model {
            id: "google/gemini-2.5-flash-lite".into(),
            name: "Gemini 2.5 Flash Lite".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.09999999999999999,
                output: 0.39999999999999997,
                cache_read: 0.01,
                cache_write: 0.0,
            },
            context_window: 1048576,
            max_tokens: 65536,
            headers: None,
            compat: None,
        },
        Model {
            id: "google/gemini-2.5-flash-lite-preview-09-2025".into(),
            name: "Gemini 2.5 Flash Lite Preview 09-2025".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.09999999999999999,
                output: 0.39999999999999997,
                cache_read: 0.01,
                cache_write: 0.0,
            },
            context_window: 1048576,
            max_tokens: 65536,
            headers: None,
            compat: None,
        },
        Model {
            id: "google/gemini-2.5-flash-preview-09-2025".into(),
            name: "Gemini 2.5 Flash Preview 09-2025".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.3,
                output: 2.5,
                cache_read: 0.03,
                cache_write: 0.0,
            },
            context_window: 1000000,
            max_tokens: 65536,
            headers: None,
            compat: None,
        },
        Model {
            id: "google/gemini-2.5-pro".into(),
            name: "Gemini 2.5 Pro".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 1.25,
                output: 10.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 1048576,
            max_tokens: 65536,
            headers: None,
            compat: None,
        },
        Model {
            id: "google/gemini-3-flash".into(),
            name: "Gemini 3 Flash".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.5,
                output: 3.0,
                cache_read: 0.049999999999999996,
                cache_write: 0.0,
            },
            context_window: 1000000,
            max_tokens: 64000,
            headers: None,
            compat: None,
        },
        Model {
            id: "google/gemini-3-pro-preview".into(),
            name: "Gemini 3 Pro Preview".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 2.0,
                output: 12.0,
                cache_read: 0.19999999999999998,
                cache_write: 0.0,
            },
            context_window: 1000000,
            max_tokens: 64000,
            headers: None,
            compat: None,
        },
        Model {
            id: "inception/mercury-coder-small".into(),
            name: "Mercury Coder Small Beta".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.25,
                output: 1.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 32000,
            max_tokens: 16384,
            headers: None,
            compat: None,
        },
        Model {
            id: "meituan/longcat-flash-chat".into(),
            name: "LongCat Flash Chat".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.0,
                output: 0.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 8192,
            headers: None,
            compat: None,
        },
        Model {
            id: "meituan/longcat-flash-thinking".into(),
            name: "LongCat Flash Thinking".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.15,
                output: 1.5,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 8192,
            headers: None,
            compat: None,
        },
        Model {
            id: "meta/llama-3.1-70b".into(),
            name: "Llama 3.1 70B Instruct".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.39999999999999997,
                output: 0.39999999999999997,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 16384,
            headers: None,
            compat: None,
        },
        Model {
            id: "meta/llama-3.1-8b".into(),
            name: "Llama 3.1 8B Instruct".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.03,
                output: 0.049999999999999996,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 16384,
            headers: None,
            compat: None,
        },
        Model {
            id: "meta/llama-3.2-11b".into(),
            name: "Llama 3.2 11B Vision Instruct".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.16,
                output: 0.16,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 8192,
            headers: None,
            compat: None,
        },
        Model {
            id: "meta/llama-3.2-90b".into(),
            name: "Llama 3.2 90B Vision Instruct".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.72,
                output: 0.72,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 8192,
            headers: None,
            compat: None,
        },
        Model {
            id: "meta/llama-3.3-70b".into(),
            name: "Llama 3.3 70B Instruct".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.72,
                output: 0.72,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 8192,
            headers: None,
            compat: None,
        },
        Model {
            id: "meta/llama-4-maverick".into(),
            name: "Llama 4 Maverick 17B Instruct".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.15,
                output: 0.6,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 8192,
            headers: None,
            compat: None,
        },
        Model {
            id: "meta/llama-4-scout".into(),
            name: "Llama 4 Scout 17B Instruct".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.08,
                output: 0.3,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 8192,
            headers: None,
            compat: None,
        },
        Model {
            id: "minimax/minimax-m2".into(),
            name: "MiniMax M2".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.3,
                output: 1.2,
                cache_read: 0.03,
                cache_write: 0.375,
            },
            context_window: 205000,
            max_tokens: 205000,
            headers: None,
            compat: None,
        },
        Model {
            id: "minimax/minimax-m2.1".into(),
            name: "MiniMax M2.1".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.3,
                output: 1.2,
                cache_read: 0.15,
                cache_write: 0.0,
            },
            context_window: 204800,
            max_tokens: 131072,
            headers: None,
            compat: None,
        },
        Model {
            id: "minimax/minimax-m2.1-lightning".into(),
            name: "MiniMax M2.1 Lightning".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.3,
                output: 2.4,
                cache_read: 0.03,
                cache_write: 0.375,
            },
            context_window: 204800,
            max_tokens: 131072,
            headers: None,
            compat: None,
        },
        Model {
            id: "mistral/codestral".into(),
            name: "Mistral Codestral".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.3,
                output: 0.8999999999999999,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 4000,
            headers: None,
            compat: None,
        },
        Model {
            id: "mistral/devstral-2".into(),
            name: "Devstral 2".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.0,
                output: 0.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 256000,
            max_tokens: 256000,
            headers: None,
            compat: None,
        },
        Model {
            id: "mistral/devstral-small".into(),
            name: "Devstral Small 1.1".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.09999999999999999,
                output: 0.3,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 64000,
            headers: None,
            compat: None,
        },
        Model {
            id: "mistral/devstral-small-2".into(),
            name: "Devstral Small 2".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.0,
                output: 0.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 256000,
            max_tokens: 256000,
            headers: None,
            compat: None,
        },
        Model {
            id: "mistral/ministral-3b".into(),
            name: "Ministral 3B".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.04,
                output: 0.04,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 4000,
            headers: None,
            compat: None,
        },
        Model {
            id: "mistral/ministral-8b".into(),
            name: "Ministral 8B".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.09999999999999999,
                output: 0.09999999999999999,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 4000,
            headers: None,
            compat: None,
        },
        Model {
            id: "mistral/mistral-medium".into(),
            name: "Mistral Medium 3.1".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.39999999999999997,
                output: 2.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 64000,
            headers: None,
            compat: None,
        },
        Model {
            id: "mistral/mistral-small".into(),
            name: "Mistral Small".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.09999999999999999,
                output: 0.3,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 32000,
            max_tokens: 4000,
            headers: None,
            compat: None,
        },
        Model {
            id: "mistral/pixtral-12b".into(),
            name: "Pixtral 12B 2409".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.15,
                output: 0.15,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 4000,
            headers: None,
            compat: None,
        },
        Model {
            id: "mistral/pixtral-large".into(),
            name: "Pixtral Large".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 2.0,
                output: 6.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 4000,
            headers: None,
            compat: None,
        },
        Model {
            id: "moonshotai/kimi-k2".into(),
            name: "Kimi K2".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.5,
                output: 2.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 16384,
            headers: None,
            compat: None,
        },
        Model {
            id: "moonshotai/kimi-k2-thinking".into(),
            name: "Kimi K2 Thinking".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.47,
                output: 2.0,
                cache_read: 0.14100000000000001,
                cache_write: 0.0,
            },
            context_window: 216144,
            max_tokens: 216144,
            headers: None,
            compat: None,
        },
        Model {
            id: "moonshotai/kimi-k2-thinking-turbo".into(),
            name: "Kimi K2 Thinking Turbo".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 1.15,
                output: 8.0,
                cache_read: 0.15,
                cache_write: 0.0,
            },
            context_window: 262114,
            max_tokens: 262114,
            headers: None,
            compat: None,
        },
        Model {
            id: "moonshotai/kimi-k2-turbo".into(),
            name: "Kimi K2 Turbo".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 2.4,
                output: 10.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 256000,
            max_tokens: 16384,
            headers: None,
            compat: None,
        },
        Model {
            id: "moonshotai/kimi-k2.5".into(),
            name: "Kimi K2.5".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.5,
                output: 2.8,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 256000,
            max_tokens: 256000,
            headers: None,
            compat: None,
        },
        Model {
            id: "nvidia/nemotron-nano-12b-v2-vl".into(),
            name: "Nvidia Nemotron Nano 12B V2 VL".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.19999999999999998,
                output: 0.6,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 131072,
            headers: None,
            compat: None,
        },
        Model {
            id: "nvidia/nemotron-nano-9b-v2".into(),
            name: "Nvidia Nemotron Nano 9B V2".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.04,
                output: 0.16,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 131072,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/codex-mini".into(),
            name: "Codex Mini".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.5,
                output: 6.0,
                cache_read: 0.375,
                cache_write: 0.0,
            },
            context_window: 200000,
            max_tokens: 100000,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/gpt-4-turbo".into(),
            name: "GPT-4 Turbo".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 10.0,
                output: 30.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/gpt-4.1".into(),
            name: "GPT-4.1".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 2.0,
                output: 8.0,
                cache_read: 0.5,
                cache_write: 0.0,
            },
            context_window: 1047576,
            max_tokens: 32768,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/gpt-4.1-mini".into(),
            name: "GPT-4.1 mini".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.39999999999999997,
                output: 1.5999999999999999,
                cache_read: 0.09999999999999999,
                cache_write: 0.0,
            },
            context_window: 1047576,
            max_tokens: 32768,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/gpt-4.1-nano".into(),
            name: "GPT-4.1 nano".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.09999999999999999,
                output: 0.39999999999999997,
                cache_read: 0.03,
                cache_write: 0.0,
            },
            context_window: 1047576,
            max_tokens: 32768,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/gpt-4o".into(),
            name: "GPT-4o".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 2.5,
                output: 10.0,
                cache_read: 1.25,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 16384,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/gpt-4o-mini".into(),
            name: "GPT-4o mini".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.15,
                output: 0.6,
                cache_read: 0.075,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 16384,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/gpt-5".into(),
            name: "GPT-5".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.25,
                output: 10.0,
                cache_read: 0.13,
                cache_write: 0.0,
            },
            context_window: 400000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/gpt-5-chat".into(),
            name: "GPT-5 Chat".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.25,
                output: 10.0,
                cache_read: 0.125,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 16384,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/gpt-5-codex".into(),
            name: "GPT-5-Codex".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.25,
                output: 10.0,
                cache_read: 0.13,
                cache_write: 0.0,
            },
            context_window: 400000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/gpt-5-mini".into(),
            name: "GPT-5 mini".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.25,
                output: 2.0,
                cache_read: 0.03,
                cache_write: 0.0,
            },
            context_window: 400000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/gpt-5-nano".into(),
            name: "GPT-5 nano".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.049999999999999996,
                output: 0.39999999999999997,
                cache_read: 0.01,
                cache_write: 0.0,
            },
            context_window: 400000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/gpt-5-pro".into(),
            name: "GPT-5 pro".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 15.0,
                output: 120.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 400000,
            max_tokens: 272000,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/gpt-5.1-codex".into(),
            name: "GPT-5.1-Codex".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.25,
                output: 10.0,
                cache_read: 0.125,
                cache_write: 0.0,
            },
            context_window: 400000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/gpt-5.1-codex-max".into(),
            name: "GPT 5.1 Codex Max".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.25,
                output: 10.0,
                cache_read: 0.125,
                cache_write: 0.0,
            },
            context_window: 400000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/gpt-5.1-codex-mini".into(),
            name: "GPT-5.1 Codex mini".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.25,
                output: 2.0,
                cache_read: 0.024999999999999998,
                cache_write: 0.0,
            },
            context_window: 400000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/gpt-5.1-instant".into(),
            name: "GPT-5.1 Instant".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.25,
                output: 10.0,
                cache_read: 0.13,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 16384,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/gpt-5.1-thinking".into(),
            name: "GPT 5.1 Thinking".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.25,
                output: 10.0,
                cache_read: 0.13,
                cache_write: 0.0,
            },
            context_window: 400000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/gpt-5.2".into(),
            name: "GPT 5.2".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.75,
                output: 14.0,
                cache_read: 0.18,
                cache_write: 0.0,
            },
            context_window: 400000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/gpt-5.2-chat".into(),
            name: "GPT-5.2 Chat".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.75,
                output: 14.0,
                cache_read: 0.175,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 16384,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/gpt-5.2-codex".into(),
            name: "GPT-5.2-Codex".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.75,
                output: 14.0,
                cache_read: 0.175,
                cache_write: 0.0,
            },
            context_window: 400000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/gpt-5.2-pro".into(),
            name: "GPT 5.2 ".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 21.0,
                output: 168.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 400000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/gpt-oss-120b".into(),
            name: "gpt-oss-120b".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.09999999999999999,
                output: 0.5,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 131072,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/gpt-oss-20b".into(),
            name: "gpt-oss-20b".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.07,
                output: 0.3,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 8192,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/gpt-oss-safeguard-20b".into(),
            name: "gpt-oss-safeguard-20b".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.075,
                output: 0.3,
                cache_read: 0.037,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 65536,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/o1".into(),
            name: "o1".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 15.0,
                output: 60.0,
                cache_read: 7.5,
                cache_write: 0.0,
            },
            context_window: 200000,
            max_tokens: 100000,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/o3".into(),
            name: "o3".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 2.0,
                output: 8.0,
                cache_read: 0.5,
                cache_write: 0.0,
            },
            context_window: 200000,
            max_tokens: 100000,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/o3-deep-research".into(),
            name: "o3-deep-research".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 10.0,
                output: 40.0,
                cache_read: 2.5,
                cache_write: 0.0,
            },
            context_window: 200000,
            max_tokens: 100000,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/o3-mini".into(),
            name: "o3-mini".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 1.1,
                output: 4.4,
                cache_read: 0.55,
                cache_write: 0.0,
            },
            context_window: 200000,
            max_tokens: 100000,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/o3-pro".into(),
            name: "o3 Pro".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 20.0,
                output: 80.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 200000,
            max_tokens: 100000,
            headers: None,
            compat: None,
        },
        Model {
            id: "openai/o4-mini".into(),
            name: "o4-mini".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.1,
                output: 4.4,
                cache_read: 0.275,
                cache_write: 0.0,
            },
            context_window: 200000,
            max_tokens: 100000,
            headers: None,
            compat: None,
        },
        Model {
            id: "perplexity/sonar".into(),
            name: "Sonar".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 1.0,
                output: 1.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 127000,
            max_tokens: 8000,
            headers: None,
            compat: None,
        },
        Model {
            id: "perplexity/sonar-pro".into(),
            name: "Sonar Pro".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 3.0,
                output: 15.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 200000,
            max_tokens: 8000,
            headers: None,
            compat: None,
        },
        Model {
            id: "prime-intellect/intellect-3".into(),
            name: "INTELLECT 3".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.19999999999999998,
                output: 1.1,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 131072,
            headers: None,
            compat: None,
        },
        Model {
            id: "vercel/v0-1.0-md".into(),
            name: "v0-1.0-md".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 3.0,
                output: 15.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 32000,
            headers: None,
            compat: None,
        },
        Model {
            id: "vercel/v0-1.5-md".into(),
            name: "v0-1.5-md".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 3.0,
                output: 15.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 32768,
            headers: None,
            compat: None,
        },
        Model {
            id: "xai/grok-2-vision".into(),
            name: "Grok 2 Vision".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 2.0,
                output: 10.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 32768,
            max_tokens: 32768,
            headers: None,
            compat: None,
        },
        Model {
            id: "xai/grok-3".into(),
            name: "Grok 3 Beta".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 3.0,
                output: 15.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 131072,
            headers: None,
            compat: None,
        },
        Model {
            id: "xai/grok-3-fast".into(),
            name: "Grok 3 Fast Beta".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 5.0,
                output: 25.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 131072,
            headers: None,
            compat: None,
        },
        Model {
            id: "xai/grok-3-mini".into(),
            name: "Grok 3 Mini Beta".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.3,
                output: 0.5,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 131072,
            headers: None,
            compat: None,
        },
        Model {
            id: "xai/grok-3-mini-fast".into(),
            name: "Grok 3 Mini Fast Beta".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.6,
                output: 4.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 131072,
            headers: None,
            compat: None,
        },
        Model {
            id: "xai/grok-4".into(),
            name: "Grok 4".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 3.0,
                output: 15.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 256000,
            max_tokens: 256000,
            headers: None,
            compat: None,
        },
        Model {
            id: "xai/grok-4-fast-non-reasoning".into(),
            name: "Grok 4 Fast Non-Reasoning".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.19999999999999998,
                output: 0.5,
                cache_read: 0.049999999999999996,
                cache_write: 0.0,
            },
            context_window: 2000000,
            max_tokens: 256000,
            headers: None,
            compat: None,
        },
        Model {
            id: "xai/grok-4-fast-reasoning".into(),
            name: "Grok 4 Fast Reasoning".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.19999999999999998,
                output: 0.5,
                cache_read: 0.049999999999999996,
                cache_write: 0.0,
            },
            context_window: 2000000,
            max_tokens: 256000,
            headers: None,
            compat: None,
        },
        Model {
            id: "xai/grok-4.1-fast-non-reasoning".into(),
            name: "Grok 4.1 Fast Non-Reasoning".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.19999999999999998,
                output: 0.5,
                cache_read: 0.049999999999999996,
                cache_write: 0.0,
            },
            context_window: 2000000,
            max_tokens: 30000,
            headers: None,
            compat: None,
        },
        Model {
            id: "xai/grok-4.1-fast-reasoning".into(),
            name: "Grok 4.1 Fast Reasoning".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.19999999999999998,
                output: 0.5,
                cache_read: 0.049999999999999996,
                cache_write: 0.0,
            },
            context_window: 2000000,
            max_tokens: 30000,
            headers: None,
            compat: None,
        },
        Model {
            id: "xai/grok-code-fast-1".into(),
            name: "Grok Code Fast 1".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.19999999999999998,
                output: 1.5,
                cache_read: 0.02,
                cache_write: 0.0,
            },
            context_window: 256000,
            max_tokens: 256000,
            headers: None,
            compat: None,
        },
        Model {
            id: "xiaomi/mimo-v2-flash".into(),
            name: "MiMo V2 Flash".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.09,
                output: 0.29,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 262144,
            max_tokens: 32000,
            headers: None,
            compat: None,
        },
        Model {
            id: "zai/glm-4.5".into(),
            name: "GLM-4.5".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.6,
                output: 2.2,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 131072,
            headers: None,
            compat: None,
        },
        Model {
            id: "zai/glm-4.5-air".into(),
            name: "GLM 4.5 Air".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.19999999999999998,
                output: 1.1,
                cache_read: 0.03,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 96000,
            headers: None,
            compat: None,
        },
        Model {
            id: "zai/glm-4.5v".into(),
            name: "GLM 4.5V".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.6,
                output: 1.7999999999999998,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 65536,
            max_tokens: 16384,
            headers: None,
            compat: None,
        },
        Model {
            id: "zai/glm-4.6".into(),
            name: "GLM 4.6".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.44999999999999996,
                output: 1.7999999999999998,
                cache_read: 0.11,
                cache_write: 0.0,
            },
            context_window: 200000,
            max_tokens: 96000,
            headers: None,
            compat: None,
        },
        Model {
            id: "zai/glm-4.6v".into(),
            name: "GLM-4.6V".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.3,
                output: 0.8999999999999999,
                cache_read: 0.049999999999999996,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 24000,
            headers: None,
            compat: None,
        },
        Model {
            id: "zai/glm-4.6v-flash".into(),
            name: "GLM-4.6V-Flash".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.0,
                output: 0.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 24000,
            headers: None,
            compat: None,
        },
        Model {
            id: "zai/glm-4.7".into(),
            name: "GLM 4.7".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.43,
                output: 1.75,
                cache_read: 0.08,
                cache_write: 0.0,
            },
            context_window: 202752,
            max_tokens: 120000,
            headers: None,
            compat: None,
        },
        Model {
            id: "zai/glm-4.7-flashx".into(),
            name: "GLM 4.7 FlashX".into(),
            api: "anthropic-messages".into(),
            provider: "vercel-ai-gateway".into(),
            base_url: "https://ai-gateway.vercel.sh".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.06,
                output: 0.39999999999999997,
                cache_read: 0.01,
                cache_write: 0.0,
            },
            context_window: 200000,
            max_tokens: 128000,
            headers: None,
            compat: None,
        },
        Model {
            id: "grok-2".into(),
            name: "Grok 2".into(),
            api: "openai-completions".into(),
            provider: "xai".into(),
            base_url: "https://api.x.ai/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 2.0,
                output: 10.0,
                cache_read: 2.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 8192,
            headers: None,
            compat: None,
        },
        Model {
            id: "grok-2-1212".into(),
            name: "Grok 2 (1212)".into(),
            api: "openai-completions".into(),
            provider: "xai".into(),
            base_url: "https://api.x.ai/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 2.0,
                output: 10.0,
                cache_read: 2.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 8192,
            headers: None,
            compat: None,
        },
        Model {
            id: "grok-2-latest".into(),
            name: "Grok 2 Latest".into(),
            api: "openai-completions".into(),
            provider: "xai".into(),
            base_url: "https://api.x.ai/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 2.0,
                output: 10.0,
                cache_read: 2.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 8192,
            headers: None,
            compat: None,
        },
        Model {
            id: "grok-2-vision".into(),
            name: "Grok 2 Vision".into(),
            api: "openai-completions".into(),
            provider: "xai".into(),
            base_url: "https://api.x.ai/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 2.0,
                output: 10.0,
                cache_read: 2.0,
                cache_write: 0.0,
            },
            context_window: 8192,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "grok-2-vision-1212".into(),
            name: "Grok 2 Vision (1212)".into(),
            api: "openai-completions".into(),
            provider: "xai".into(),
            base_url: "https://api.x.ai/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 2.0,
                output: 10.0,
                cache_read: 2.0,
                cache_write: 0.0,
            },
            context_window: 8192,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "grok-2-vision-latest".into(),
            name: "Grok 2 Vision Latest".into(),
            api: "openai-completions".into(),
            provider: "xai".into(),
            base_url: "https://api.x.ai/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 2.0,
                output: 10.0,
                cache_read: 2.0,
                cache_write: 0.0,
            },
            context_window: 8192,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "grok-3".into(),
            name: "Grok 3".into(),
            api: "openai-completions".into(),
            provider: "xai".into(),
            base_url: "https://api.x.ai/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 3.0,
                output: 15.0,
                cache_read: 0.75,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 8192,
            headers: None,
            compat: None,
        },
        Model {
            id: "grok-3-fast".into(),
            name: "Grok 3 Fast".into(),
            api: "openai-completions".into(),
            provider: "xai".into(),
            base_url: "https://api.x.ai/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 5.0,
                output: 25.0,
                cache_read: 1.25,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 8192,
            headers: None,
            compat: None,
        },
        Model {
            id: "grok-3-fast-latest".into(),
            name: "Grok 3 Fast Latest".into(),
            api: "openai-completions".into(),
            provider: "xai".into(),
            base_url: "https://api.x.ai/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 5.0,
                output: 25.0,
                cache_read: 1.25,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 8192,
            headers: None,
            compat: None,
        },
        Model {
            id: "grok-3-latest".into(),
            name: "Grok 3 Latest".into(),
            api: "openai-completions".into(),
            provider: "xai".into(),
            base_url: "https://api.x.ai/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 3.0,
                output: 15.0,
                cache_read: 0.75,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 8192,
            headers: None,
            compat: None,
        },
        Model {
            id: "grok-3-mini".into(),
            name: "Grok 3 Mini".into(),
            api: "openai-completions".into(),
            provider: "xai".into(),
            base_url: "https://api.x.ai/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.3,
                output: 0.5,
                cache_read: 0.075,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 8192,
            headers: None,
            compat: None,
        },
        Model {
            id: "grok-3-mini-fast".into(),
            name: "Grok 3 Mini Fast".into(),
            api: "openai-completions".into(),
            provider: "xai".into(),
            base_url: "https://api.x.ai/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.6,
                output: 4.0,
                cache_read: 0.15,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 8192,
            headers: None,
            compat: None,
        },
        Model {
            id: "grok-3-mini-fast-latest".into(),
            name: "Grok 3 Mini Fast Latest".into(),
            api: "openai-completions".into(),
            provider: "xai".into(),
            base_url: "https://api.x.ai/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.6,
                output: 4.0,
                cache_read: 0.15,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 8192,
            headers: None,
            compat: None,
        },
        Model {
            id: "grok-3-mini-latest".into(),
            name: "Grok 3 Mini Latest".into(),
            api: "openai-completions".into(),
            provider: "xai".into(),
            base_url: "https://api.x.ai/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.3,
                output: 0.5,
                cache_read: 0.075,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 8192,
            headers: None,
            compat: None,
        },
        Model {
            id: "grok-4".into(),
            name: "Grok 4".into(),
            api: "openai-completions".into(),
            provider: "xai".into(),
            base_url: "https://api.x.ai/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 3.0,
                output: 15.0,
                cache_read: 0.75,
                cache_write: 0.0,
            },
            context_window: 256000,
            max_tokens: 64000,
            headers: None,
            compat: None,
        },
        Model {
            id: "grok-4-1-fast".into(),
            name: "Grok 4.1 Fast".into(),
            api: "openai-completions".into(),
            provider: "xai".into(),
            base_url: "https://api.x.ai/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.2,
                output: 0.5,
                cache_read: 0.05,
                cache_write: 0.0,
            },
            context_window: 2000000,
            max_tokens: 30000,
            headers: None,
            compat: None,
        },
        Model {
            id: "grok-4-1-fast-non-reasoning".into(),
            name: "Grok 4.1 Fast (Non-Reasoning)".into(),
            api: "openai-completions".into(),
            provider: "xai".into(),
            base_url: "https://api.x.ai/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.2,
                output: 0.5,
                cache_read: 0.05,
                cache_write: 0.0,
            },
            context_window: 2000000,
            max_tokens: 30000,
            headers: None,
            compat: None,
        },
        Model {
            id: "grok-4-fast".into(),
            name: "Grok 4 Fast".into(),
            api: "openai-completions".into(),
            provider: "xai".into(),
            base_url: "https://api.x.ai/v1".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.2,
                output: 0.5,
                cache_read: 0.05,
                cache_write: 0.0,
            },
            context_window: 2000000,
            max_tokens: 30000,
            headers: None,
            compat: None,
        },
        Model {
            id: "grok-4-fast-non-reasoning".into(),
            name: "Grok 4 Fast (Non-Reasoning)".into(),
            api: "openai-completions".into(),
            provider: "xai".into(),
            base_url: "https://api.x.ai/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.2,
                output: 0.5,
                cache_read: 0.05,
                cache_write: 0.0,
            },
            context_window: 2000000,
            max_tokens: 30000,
            headers: None,
            compat: None,
        },
        Model {
            id: "grok-beta".into(),
            name: "Grok Beta".into(),
            api: "openai-completions".into(),
            provider: "xai".into(),
            base_url: "https://api.x.ai/v1".into(),
            reasoning: false,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 5.0,
                output: 15.0,
                cache_read: 5.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "grok-code-fast-1".into(),
            name: "Grok Code Fast 1".into(),
            api: "openai-completions".into(),
            provider: "xai".into(),
            base_url: "https://api.x.ai/v1".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.2,
                output: 1.5,
                cache_read: 0.02,
                cache_write: 0.0,
            },
            context_window: 256000,
            max_tokens: 10000,
            headers: None,
            compat: None,
        },
        Model {
            id: "grok-vision-beta".into(),
            name: "Grok Vision Beta".into(),
            api: "openai-completions".into(),
            provider: "xai".into(),
            base_url: "https://api.x.ai/v1".into(),
            reasoning: false,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 5.0,
                output: 15.0,
                cache_read: 5.0,
                cache_write: 0.0,
            },
            context_window: 8192,
            max_tokens: 4096,
            headers: None,
            compat: None,
        },
        Model {
            id: "glm-4.5".into(),
            name: "GLM-4.5".into(),
            api: "openai-completions".into(),
            provider: "zai".into(),
            base_url: "https://api.z.ai/api/coding/paas/v4".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.6,
                output: 2.2,
                cache_read: 0.11,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 98304,
            headers: None,
            compat: Some(json!({"supportsDeveloperRole":false,"thinkingFormat":"zai"})),
        },
        Model {
            id: "glm-4.5-air".into(),
            name: "GLM-4.5-Air".into(),
            api: "openai-completions".into(),
            provider: "zai".into(),
            base_url: "https://api.z.ai/api/coding/paas/v4".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.2,
                output: 1.1,
                cache_read: 0.03,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 98304,
            headers: None,
            compat: Some(json!({"supportsDeveloperRole":false,"thinkingFormat":"zai"})),
        },
        Model {
            id: "glm-4.5-flash".into(),
            name: "GLM-4.5-Flash".into(),
            api: "openai-completions".into(),
            provider: "zai".into(),
            base_url: "https://api.z.ai/api/coding/paas/v4".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.0,
                output: 0.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 131072,
            max_tokens: 98304,
            headers: None,
            compat: Some(json!({"supportsDeveloperRole":false,"thinkingFormat":"zai"})),
        },
        Model {
            id: "glm-4.5v".into(),
            name: "GLM-4.5V".into(),
            api: "openai-completions".into(),
            provider: "zai".into(),
            base_url: "https://api.z.ai/api/coding/paas/v4".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.6,
                output: 1.8,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 64000,
            max_tokens: 16384,
            headers: None,
            compat: Some(json!({"supportsDeveloperRole":false,"thinkingFormat":"zai"})),
        },
        Model {
            id: "glm-4.6".into(),
            name: "GLM-4.6".into(),
            api: "openai-completions".into(),
            provider: "zai".into(),
            base_url: "https://api.z.ai/api/coding/paas/v4".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.6,
                output: 2.2,
                cache_read: 0.11,
                cache_write: 0.0,
            },
            context_window: 204800,
            max_tokens: 131072,
            headers: None,
            compat: Some(json!({"supportsDeveloperRole":false,"thinkingFormat":"zai"})),
        },
        Model {
            id: "glm-4.6v".into(),
            name: "GLM-4.6V".into(),
            api: "openai-completions".into(),
            provider: "zai".into(),
            base_url: "https://api.z.ai/api/coding/paas/v4".into(),
            reasoning: true,
            input: vec!["text".into(), "image".into()],
            cost: ModelCost {
                input: 0.3,
                output: 0.9,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 128000,
            max_tokens: 32768,
            headers: None,
            compat: Some(json!({"supportsDeveloperRole":false,"thinkingFormat":"zai"})),
        },
        Model {
            id: "glm-4.7".into(),
            name: "GLM-4.7".into(),
            api: "openai-completions".into(),
            provider: "zai".into(),
            base_url: "https://api.z.ai/api/coding/paas/v4".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.6,
                output: 2.2,
                cache_read: 0.11,
                cache_write: 0.0,
            },
            context_window: 204800,
            max_tokens: 131072,
            headers: None,
            compat: Some(json!({"supportsDeveloperRole":false,"thinkingFormat":"zai"})),
        },
        Model {
            id: "glm-4.7-flash".into(),
            name: "GLM-4.7-Flash".into(),
            api: "openai-completions".into(),
            provider: "zai".into(),
            base_url: "https://api.z.ai/api/coding/paas/v4".into(),
            reasoning: true,
            input: vec!["text".into()],
            cost: ModelCost {
                input: 0.0,
                output: 0.0,
                cache_read: 0.0,
                cache_write: 0.0,
            },
            context_window: 200000,
            max_tokens: 131072,
            headers: None,
            compat: Some(json!({"supportsDeveloperRole":false,"thinkingFormat":"zai"})),
        },
    ]
}
